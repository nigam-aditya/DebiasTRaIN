{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "454f6ab1a5ff43f69abf92a3254b6ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b98c2d9d84bc445e8adf1a7c777b3ba2",
              "IPY_MODEL_e1975eb07d0b4c0eb22060196eb220cb",
              "IPY_MODEL_08e6880c722441769ef0213de9594f37"
            ],
            "layout": "IPY_MODEL_c41ca4d590ab474787802a1a63971857"
          }
        },
        "b98c2d9d84bc445e8adf1a7c777b3ba2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d79a594fb2d4fe2b72cbb72d8e73e8d",
            "placeholder": "​",
            "style": "IPY_MODEL_d663424161634f808a52630b50ceb6a9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e1975eb07d0b4c0eb22060196eb220cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03b3485372cd4045aa7eba416252a80c",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c700bcdfb15433d8a0fb8f9d2c7459c",
            "value": 48
          }
        },
        "08e6880c722441769ef0213de9594f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d13c9e57b5ac4b9d9b45e518d3a7deb4",
            "placeholder": "​",
            "style": "IPY_MODEL_ed66838de2ac4799a471b91f49a43ab0",
            "value": " 48.0/48.0 [00:00&lt;00:00, 4.87kB/s]"
          }
        },
        "c41ca4d590ab474787802a1a63971857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d79a594fb2d4fe2b72cbb72d8e73e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d663424161634f808a52630b50ceb6a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "03b3485372cd4045aa7eba416252a80c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c700bcdfb15433d8a0fb8f9d2c7459c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d13c9e57b5ac4b9d9b45e518d3a7deb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed66838de2ac4799a471b91f49a43ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70332968949a4e7893369f83d388d081": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63a7f617ca8b46fe9df5e9b951e50838",
              "IPY_MODEL_b17e24fdd56940a699a205e924822960",
              "IPY_MODEL_3606c92eb21b4c6a9e6ec3c8b2e7f0b1"
            ],
            "layout": "IPY_MODEL_29e8650b0d6842ee81c89b4e59ec2b6d"
          }
        },
        "63a7f617ca8b46fe9df5e9b951e50838": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0906ecfc4e1c47cb8a648cf0e5fb6c17",
            "placeholder": "​",
            "style": "IPY_MODEL_ffea2e2b00e0416e848ddb6b7f75c34d",
            "value": "config.json: 100%"
          }
        },
        "b17e24fdd56940a699a205e924822960": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc511e0997ac437ab537efe9f2b76c50",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_696c166775af493892a227413098013d",
            "value": 570
          }
        },
        "3606c92eb21b4c6a9e6ec3c8b2e7f0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f917b14a8c7148c59ad3659e43241ab9",
            "placeholder": "​",
            "style": "IPY_MODEL_5ca0f3173d0f49e18ab4ac06b304907a",
            "value": " 570/570 [00:00&lt;00:00, 61.5kB/s]"
          }
        },
        "29e8650b0d6842ee81c89b4e59ec2b6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0906ecfc4e1c47cb8a648cf0e5fb6c17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffea2e2b00e0416e848ddb6b7f75c34d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bc511e0997ac437ab537efe9f2b76c50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696c166775af493892a227413098013d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f917b14a8c7148c59ad3659e43241ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ca0f3173d0f49e18ab4ac06b304907a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1718cd4f479e40509ff8b31cdd24b131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48c2d25f22e94fefb0c978461e84fb41",
              "IPY_MODEL_8ed8c5c53ffd4c708f0adbd03a812c42",
              "IPY_MODEL_919ee9d194274c0e8ca6431650d334a1"
            ],
            "layout": "IPY_MODEL_7b4b40fc099f41b8831bef725d59bf49"
          }
        },
        "48c2d25f22e94fefb0c978461e84fb41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff6eabe82d10413bb6126039d73bdbc0",
            "placeholder": "​",
            "style": "IPY_MODEL_4005d4496d2948148dd6352cfc875206",
            "value": "vocab.txt: 100%"
          }
        },
        "8ed8c5c53ffd4c708f0adbd03a812c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0cdde97609d4fd68d53bd90356617bf",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a429d2d074a04fe9be050f862ab089d2",
            "value": 231508
          }
        },
        "919ee9d194274c0e8ca6431650d334a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_091076825dfc4467862eb9d37a1a50d2",
            "placeholder": "​",
            "style": "IPY_MODEL_22f0a6de43fd4202865ab12dd26b9779",
            "value": " 232k/232k [00:00&lt;00:00, 3.38MB/s]"
          }
        },
        "7b4b40fc099f41b8831bef725d59bf49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff6eabe82d10413bb6126039d73bdbc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4005d4496d2948148dd6352cfc875206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0cdde97609d4fd68d53bd90356617bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a429d2d074a04fe9be050f862ab089d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "091076825dfc4467862eb9d37a1a50d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22f0a6de43fd4202865ab12dd26b9779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ec8a241034d4f43873300cd918fd86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed4d28f9bccd42fd8aaefbabb2c4e557",
              "IPY_MODEL_1c30d34ed5fb41c6ac3b8a3afddcdec8",
              "IPY_MODEL_dff9ad23dbcf48b5b14c4b66aae56726"
            ],
            "layout": "IPY_MODEL_c321e4d2fb4d430eb6b6cb0b142e5207"
          }
        },
        "ed4d28f9bccd42fd8aaefbabb2c4e557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13753f6ac54d4ace89203721158f34f1",
            "placeholder": "​",
            "style": "IPY_MODEL_4b16dce1f1f6471bb22ec0551f21cb64",
            "value": "tokenizer.json: 100%"
          }
        },
        "1c30d34ed5fb41c6ac3b8a3afddcdec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aeb634a9e7a9439aac28ee4f5b9a805d",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ccc14e243a3424b955b359139a09286",
            "value": 466062
          }
        },
        "dff9ad23dbcf48b5b14c4b66aae56726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04459da47c8a4e86945a1ca3a3abd11c",
            "placeholder": "​",
            "style": "IPY_MODEL_f94cd458d5454eb0b47591689160aa1d",
            "value": " 466k/466k [00:00&lt;00:00, 7.10MB/s]"
          }
        },
        "c321e4d2fb4d430eb6b6cb0b142e5207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13753f6ac54d4ace89203721158f34f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b16dce1f1f6471bb22ec0551f21cb64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aeb634a9e7a9439aac28ee4f5b9a805d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ccc14e243a3424b955b359139a09286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "04459da47c8a4e86945a1ca3a3abd11c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f94cd458d5454eb0b47591689160aa1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab907390de794177b3700c148663fad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22163cf4a42d4f969c7d99cef5b75fa1",
              "IPY_MODEL_ca2dde34d2de4414aadd5e9c6a11099a",
              "IPY_MODEL_644632eb2d7646e5a3c41fa8ca604a04"
            ],
            "layout": "IPY_MODEL_dd54543e1a3549cf9d3140dc7103703c"
          }
        },
        "22163cf4a42d4f969c7d99cef5b75fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_328b1da9c1de44ad859df4abc23e4248",
            "placeholder": "​",
            "style": "IPY_MODEL_8bae81bf735e4d00a3a281f07d0ab3c8",
            "value": "model.safetensors: 100%"
          }
        },
        "ca2dde34d2de4414aadd5e9c6a11099a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb9bcf94fa9943fcb5b9de19f322a5ee",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b7e5f2f3ba448bb91e4d47ed4570f28",
            "value": 440449768
          }
        },
        "644632eb2d7646e5a3c41fa8ca604a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5347af11276142c485cfcdbc6e096168",
            "placeholder": "​",
            "style": "IPY_MODEL_40f95a6d0c49429c93b646990dd2df79",
            "value": " 440M/440M [00:08&lt;00:00, 34.2MB/s]"
          }
        },
        "dd54543e1a3549cf9d3140dc7103703c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "328b1da9c1de44ad859df4abc23e4248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bae81bf735e4d00a3a281f07d0ab3c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb9bcf94fa9943fcb5b9de19f322a5ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b7e5f2f3ba448bb91e4d47ed4570f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5347af11276142c485cfcdbc6e096168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40f95a6d0c49429c93b646990dd2df79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing Packages"
      ],
      "metadata": {
        "id": "vPRksE2lfrUV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF-sVFEtjHKR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets\n",
        "!pip install accelerate einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from string import Template\n",
        "import random\n",
        "import json\n"
      ],
      "metadata": {
        "id": "5evCgu4Rjy0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Import from Hugging Face"
      ],
      "metadata": {
        "id": "xwwDFQzVf_kD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"bert-base-uncased\""
      ],
      "metadata": {
        "id": "pC_1ZZry4Emr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353,
          "referenced_widgets": [
            "454f6ab1a5ff43f69abf92a3254b6ebf",
            "b98c2d9d84bc445e8adf1a7c777b3ba2",
            "e1975eb07d0b4c0eb22060196eb220cb",
            "08e6880c722441769ef0213de9594f37",
            "c41ca4d590ab474787802a1a63971857",
            "0d79a594fb2d4fe2b72cbb72d8e73e8d",
            "d663424161634f808a52630b50ceb6a9",
            "03b3485372cd4045aa7eba416252a80c",
            "5c700bcdfb15433d8a0fb8f9d2c7459c",
            "d13c9e57b5ac4b9d9b45e518d3a7deb4",
            "ed66838de2ac4799a471b91f49a43ab0",
            "70332968949a4e7893369f83d388d081",
            "63a7f617ca8b46fe9df5e9b951e50838",
            "b17e24fdd56940a699a205e924822960",
            "3606c92eb21b4c6a9e6ec3c8b2e7f0b1",
            "29e8650b0d6842ee81c89b4e59ec2b6d",
            "0906ecfc4e1c47cb8a648cf0e5fb6c17",
            "ffea2e2b00e0416e848ddb6b7f75c34d",
            "bc511e0997ac437ab537efe9f2b76c50",
            "696c166775af493892a227413098013d",
            "f917b14a8c7148c59ad3659e43241ab9",
            "5ca0f3173d0f49e18ab4ac06b304907a",
            "1718cd4f479e40509ff8b31cdd24b131",
            "48c2d25f22e94fefb0c978461e84fb41",
            "8ed8c5c53ffd4c708f0adbd03a812c42",
            "919ee9d194274c0e8ca6431650d334a1",
            "7b4b40fc099f41b8831bef725d59bf49",
            "ff6eabe82d10413bb6126039d73bdbc0",
            "4005d4496d2948148dd6352cfc875206",
            "f0cdde97609d4fd68d53bd90356617bf",
            "a429d2d074a04fe9be050f862ab089d2",
            "091076825dfc4467862eb9d37a1a50d2",
            "22f0a6de43fd4202865ab12dd26b9779",
            "1ec8a241034d4f43873300cd918fd86b",
            "ed4d28f9bccd42fd8aaefbabb2c4e557",
            "1c30d34ed5fb41c6ac3b8a3afddcdec8",
            "dff9ad23dbcf48b5b14c4b66aae56726",
            "c321e4d2fb4d430eb6b6cb0b142e5207",
            "13753f6ac54d4ace89203721158f34f1",
            "4b16dce1f1f6471bb22ec0551f21cb64",
            "aeb634a9e7a9439aac28ee4f5b9a805d",
            "4ccc14e243a3424b955b359139a09286",
            "04459da47c8a4e86945a1ca3a3abd11c",
            "f94cd458d5454eb0b47591689160aa1d",
            "ab907390de794177b3700c148663fad5",
            "22163cf4a42d4f969c7d99cef5b75fa1",
            "ca2dde34d2de4414aadd5e9c6a11099a",
            "644632eb2d7646e5a3c41fa8ca604a04",
            "dd54543e1a3549cf9d3140dc7103703c",
            "328b1da9c1de44ad859df4abc23e4248",
            "8bae81bf735e4d00a3a281f07d0ab3c8",
            "bb9bcf94fa9943fcb5b9de19f322a5ee",
            "7b7e5f2f3ba448bb91e4d47ed4570f28",
            "5347af11276142c485cfcdbc6e096168",
            "40f95a6d0c49429c93b646990dd2df79"
          ]
        },
        "id": "sTwioGQRmTIj",
        "outputId": "ba5360d6-ac54-4a83-d5b7-8e7d1a06ba6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "454f6ab1a5ff43f69abf92a3254b6ebf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70332968949a4e7893369f83d388d081"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1718cd4f479e40509ff8b31cdd24b131"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ec8a241034d4f43873300cd918fd86b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab907390de794177b3700c148663fad5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Templates"
      ],
      "metadata": {
        "id": "47_uc1ZogIMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bias_template import prepare_dataset_for_masked_model"
      ],
      "metadata": {
        "id": "7UUrboTPlyqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "occupation_dataset = prepare_dataset_for_masked_model(tokenizer, return_unencoded_sentences=False, model=model.to(\"cpu\"))\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        k: torch.tensor([item[k] for item in batch]).to(device)  # Move to device here\n",
        "        for k in batch[0].keys()\n",
        "    }\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "train_loader = DataLoader(occupation_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHhD53dcj0HM",
        "outputId": "aa7274ba-05d3-40be-b21d-ce62afa08bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Option 'MP' is not a valid token\n",
            "WARNING:root:Option 'DJ' is not a valid token\n",
            "WARNING:root:Option 'Engineer' is not a valid token\n",
            "WARNING:root:Option 'Doctor' is not a valid token\n",
            "WARNING:root:Option 'MD' is not a valid token\n",
            "WARNING:root:Option 'Architect' is not a valid token\n",
            "WARNING:root:Option 'Professor' is not a valid token\n",
            "WARNING:root:Option 'PhD' is not a valid token\n",
            "WARNING:root:Option 'Scientist' is not a valid token\n",
            "WARNING:root:Option 'Chef' is not a valid token\n",
            "WARNING:root:Option 'MBA' is not a valid token\n",
            "WARNING:root:Option 'Officer' is not a valid token\n",
            "WARNING:root:Option 'Cook' is not a valid token\n",
            "WARNING:root:Option 'GP' is not a valid token\n",
            "WARNING:root:Option 'MLA' is not a valid token\n",
            "WARNING:root:Option 'MB' is not a valid token\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function"
      ],
      "metadata": {
        "id": "L6w29YVygQ8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_equal_valid_options_mask_logits(male_logits, female_logits, output_indices):\n",
        "    # logits: [batch, vocab_size]\n",
        "    loss = 0.0\n",
        "    for i in range(len(male_logits)):\n",
        "        indices = output_indices[i]\n",
        "        m_logits = male_logits[i][indices]\n",
        "        f_logits = female_logits[i][indices]\n",
        "        loss += torch.mean((m_logits - f_logits) ** 2)\n",
        "    return loss / len(male_logits)"
      ],
      "metadata": {
        "id": "pCC8cofXmdJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function for implementing SODA's continuous relaxation"
      ],
      "metadata": {
        "id": "UjRFSHP9gTwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedded_prompt(soft_prompt_logits, model):\n",
        "    # Turn soft one-hot into embeddings via weighted sum\n",
        "    embedding_matrix = model.get_input_embeddings().weight.detach()  # [vocab_size, hidden_size]\n",
        "    probs = F.softmax(soft_prompt_logits, dim=-1)  # [prompt_len, vocab_size]\n",
        "    return probs @ embedding_matrix  # [prompt_length, hidden_dim]"
      ],
      "metadata": {
        "id": "HEYHRTfOnw4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Custom Adam Optimizer from SODA"
      ],
      "metadata": {
        "id": "FLPM-hgQgoBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAdam(torch.optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps)\n",
        "        super(CustomAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError(\"Adam does not support sparse gradients\")\n",
        "\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)  # First moment (m_t)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)  # Second moment (v_t)\n",
        "\n",
        "                m, v = state['exp_avg'], state['exp_avg_sq']\n",
        "                beta1, beta2 = group['betas']\n",
        "                state['step'] += 1\n",
        "                t = state['step']\n",
        "\n",
        "                m.mul_(beta1).add_(grad, alpha=1 - beta1) # m_t = β1 * m_{t-1} + (1 - β1) * g_t\n",
        "                v.mul_(beta2).addcmul_(grad, grad, value=1 - beta2) # v_t = β2 * v_{t-1} + (1 - β2) * g_t^2\n",
        "                # m_hat = m / (1 - beta1**t) # m̂_t = m_t / (1 - β1^t)\n",
        "                # v_hat = v / (1 - beta2**t) # v̂_t = v_t / (1 - β2^t)\n",
        "                m_hat = m # m̂_t = m_t\n",
        "                v_hat = v # v̂_t = v_t\n",
        "                denom = v_hat.sqrt().add(group['eps'])\n",
        "                p.data.addcdiv_(m_h at, denom, value=-group['lr']) # θ_t = θ_{t-1} - η * m̂_t / (sqrt(v̂_t) + ε)\n",
        "\n",
        "                '''\n",
        "                Funky stuff\n",
        "                '''\n",
        "\n",
        "                # m.mul_(beta1).add_(grad, alpha=1 - beta1) # m_t = β1 * m_{t-1} + (1 - β1) * g_t\n",
        "                # m_hat = m # m̂_t = m_t\n",
        "                # p.data.add_(m_hat.sign(), alpha=-group['lr']) # θ_t = θ_{t-1} - η * sign(m̂_t)\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "u7_hkTAu8sBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Embedding layer device: {model.get_input_embeddings().weight.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbwpixaJ0FLl",
        "outputId": "3ee302ed-c10d-4dbd-b862-58269308a5a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model device: cuda:0\n",
            "Embedding layer device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78nXf9OZXAcY",
        "outputId": "cacee866-9c96-49e6-cadd-214bec4d10fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import GPT-2 model trained on wikitext data using BERT tokenizer for incorporating fluency loss"
      ],
      "metadata": {
        "id": "8IjunzXqgudT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L \"https://dl.fbaipublicfiles.com/text-adversarial-attack/transformer_wikitext-103.pth\" -o \"/content/drive/My Drive/transformer_wikitext-103.pth\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AVGBBy4GhT7",
        "outputId": "405f16d0-ad2d-404a-80a7-f7e9b7770b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1419M  100 1419M    0     0  72.2M      0  0:00:19  0:00:19 --:--:-- 77.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/transformer_wikitext-103.pth'"
      ],
      "metadata": {
        "id": "WLD50rSwXSp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embedding_from_weights(w):\n",
        "    layer = torch.nn.Embedding(w.size(0), w.size(1))\n",
        "    layer.weight.data = w\n",
        "\n",
        "    return layer\n",
        "\n",
        "def load_gpt2_from_dict(dict_path, output_hidden_states=False):\n",
        "    state_dict = torch.load(dict_path)['model']\n",
        "\n",
        "    config = GPT2Config(\n",
        "        vocab_size=30522,\n",
        "        n_embd=1024,\n",
        "        n_head=8,\n",
        "        activation_function='relu',\n",
        "        n_layer=24,\n",
        "        output_hidden_states=output_hidden_states\n",
        "    )\n",
        "    model = GPT2LMHeadModel(config)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    # The input embedding is not loaded automatically\n",
        "    model.set_input_embeddings(embedding_from_weights(state_dict['transformer.wte.weight'].cpu()))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "R6a5Bq6GT4G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_model = load_gpt2_from_dict(path).cuda()"
      ],
      "metadata": {
        "id": "uveXBOihWJKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fluency Loss"
      ],
      "metadata": {
        "id": "c5WCtrJbhIZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_fluency(soft_logits, ref_model):\n",
        "  \"\"\"\n",
        "  Single-pass version of  L = -Σ_{i=1}^{T-1}  π_{i+1} · log p_g(·|π_{≤i})\n",
        "  \"\"\"\n",
        "\n",
        "  # 1) soft token distributions π_i\n",
        "  probs = F.softmax(soft_logits, dim=-1)  # [prompt_len, vocab_size] = [T,V]\n",
        "  prompt_len, vocab_size = soft_logits.shape\n",
        "  device = soft_logits.device\n",
        "\n",
        "  # 2) embed whole prompt\n",
        "  emb_mat = ref_model.transformer.wte.weight         # [V,D]\n",
        "  embeds = (probs @ emb_mat).unsqueeze(0)  # [1, prompt_length, hidden_dim] = [1,T,D]\n",
        "\n",
        "\n",
        "  # 3) forward once\n",
        "  logits  = ref_model(inputs_embeds=embeds).logits   # [1,T,V]\n",
        "  logp    = F.log_softmax(logits[:, :-1, :], dim=-1) # predicts tokens 2‥T  [1,T-1,V]\n",
        "\n",
        "  # 4) targets π₂ … π_T\n",
        "  targets = probs[1:, :]                             # [T-1,V]\n",
        "\n",
        "  # 5) summed cross-entropy\n",
        "  flu_loss = -(targets * logp.squeeze(0)).sum()      # scalar\n",
        "\n",
        "  return flu_loss/(prompt_len -1)"
      ],
      "metadata": {
        "id": "sEk9_bepTreD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Construction using Debias TRaIN"
      ],
      "metadata": {
        "id": "3ab7P0czhi0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_length = 10\n",
        "vocab_size = tokenizer.vocab_size\n",
        "soft_prompt = nn.Parameter(torch.zeros(prompt_length, vocab_size,device=device), requires_grad=True)\n",
        "\n",
        "#optimizer = AdamW([soft_prompt], lr=1e-2)\n",
        "optimizer = CustomAdam([soft_prompt], lr=4e-3)\n",
        "\n",
        "EPOCHS = 50\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    total_main_loss = 0\n",
        "    total_fluency_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        '''\n",
        "        # Debug: Print device info for all tensors\n",
        "        print(\"Checking devices:\")\n",
        "        for k, v in batch.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                print(f\"{k}: {v.device}\")\n",
        "            elif isinstance(v, list) and len(v) > 0 and isinstance(v[0], torch.Tensor):\n",
        "                print(f\"{k}: {v[0].device} (list of tensors)\")\n",
        "        '''\n",
        "\n",
        "        model.zero_grad()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute soft prompt embedding\n",
        "        input_ids_m = batch[\"input_ids_male\"]\n",
        "        input_ids_f = batch[\"input_ids_female\"]\n",
        "        attn_mask_m = batch[\"attention_mask_male\"]\n",
        "        attn_mask_f = batch[\"attention_mask_female\"]\n",
        "        mask_idx_m = batch[\"mask_token_idx_male\"] + prompt_length\n",
        "        mask_idx_f = batch[\"mask_token_idx_female\"] + prompt_length\n",
        "        output_indices = batch[\"output_indices\"]\n",
        "\n",
        "        # Embed input + prefix\n",
        "        inputs_embeds_m = model.get_input_embeddings()(input_ids_m)\n",
        "        inputs_embeds_f = model.get_input_embeddings()(input_ids_f)\n",
        "\n",
        "        embedded_prompt = get_embedded_prompt(soft_prompt, model)\n",
        "\n",
        "        # Extend attention masks first\n",
        "        prompt_length = embedded_prompt.size(0)\n",
        "        attn_mask_m = torch.cat([\n",
        "            torch.ones((attn_mask_m.size(0), prompt_length),\n",
        "                      dtype=attn_mask_m.dtype,\n",
        "                      device=device),\n",
        "            attn_mask_m\n",
        "        ], dim=1)\n",
        "        attn_mask_f = torch.cat([\n",
        "            torch.ones((attn_mask_f.size(0), prompt_length),\n",
        "                      dtype=attn_mask_f.dtype,\n",
        "                      device=device),\n",
        "            attn_mask_f\n",
        "        ], dim=1)\n",
        "\n",
        "        inputs_embeds_m = torch.cat([embedded_prompt.unsqueeze(0).expand(inputs_embeds_m.size(0), -1, -1), inputs_embeds_m], dim=1)\n",
        "        inputs_embeds_f = torch.cat([embedded_prompt.unsqueeze(0).expand(inputs_embeds_f.size(0), -1, -1), inputs_embeds_f], dim=1)\n",
        "\n",
        "        outputs_m = model(attention_mask=attn_mask_m, inputs_embeds=inputs_embeds_m)\n",
        "        outputs_f = model(attention_mask=attn_mask_f, inputs_embeds=inputs_embeds_f)\n",
        "\n",
        "        logits_m = outputs_m.logits[torch.arange(len(mask_idx_m)), mask_idx_m]\n",
        "        logits_f = outputs_f.logits[torch.arange(len(mask_idx_f)), mask_idx_f]\n",
        "\n",
        "        loss = loss_equal_valid_options_mask_logits(logits_m, logits_f, output_indices)\n",
        "\n",
        "        #Fluency\n",
        "\n",
        "        #fluency_loss = causal_fluency(soft_prompt,ref_model)\n",
        "\n",
        "        '''\n",
        "        #seperate LRS\n",
        "        # Set desired learning rates\n",
        "        lr_b = 1e-2     # effective learning rate for bias loss\n",
        "        lr_f = 5e-2     # effective learning rate for fluency loss\n",
        "\n",
        "        # Backpropagate each loss separately\n",
        "        (loss_main * lr_b).backward(retain_graph=True)\n",
        "        if(epoch>10):\n",
        "          (fluency_loss * lr_f).backward()\n",
        "        '''\n",
        "\n",
        "        # Combine losses\n",
        "        #lambda_fluency = 0.05  # Tune this weight\n",
        "        #lambda_bias =  2 # Tune this weight\n",
        "        #lambda_GPT = 0.1\n",
        "\n",
        "        '''\n",
        "        if(epoch<10):\n",
        "          loss = loss_main\n",
        "        else:\n",
        "        '''\n",
        "        #loss = loss_main + lambda_fluency * fluency_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #total_main_loss += loss_main.item()\n",
        "        #total_fluency_loss += fluency_loss.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "    top_tokens = torch.argmax(soft_prompt, dim=-1)  # [prompt_length]\n",
        "    prompt_tokens=top_tokens.tolist()\n",
        "    discrete_prompt = tokenizer.convert_ids_to_tokens(prompt_tokens)\n",
        "    print(\"Discrete prompt:\", discrete_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj3TTB9Bn4cj",
        "outputId": "c5c8da75-36c8-429a-85f5-e9fbf673b952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 5.7280\n",
            "Discrete prompt: ['##gun', 'wikipedia', 'hon', '##won', '##ф', 'knots', '##lone', 'certified', 'word', 'autobiographical']\n",
            "Epoch 2/50, Loss: 5.0522\n",
            "Discrete prompt: ['carey', 'geek', 'aunt', 'mum', '##sund', 'girlfriends', '##sund', '##orous', 'outright', 'penalties']\n",
            "Epoch 3/50, Loss: 4.6329\n",
            "Discrete prompt: ['##rock', 'fork', 'aunt', 'mum', '##sund', 'girlfriends', '##sund', 'roommate', 'outright', 'penalties']\n",
            "Epoch 4/50, Loss: 4.3294\n",
            "Discrete prompt: ['crystal', 'fork', 'aunt', 'mum', '##sund', 'girlfriends', 'charting', 'roommate', 'outright', 'penalties']\n",
            "Epoch 5/50, Loss: 4.0112\n",
            "Discrete prompt: ['crystal', 'web', 'aunt', 'mum', 'minsk', 'girlfriends', 'primate', 'roommate', 'outright', 'penalties']\n",
            "Epoch 6/50, Loss: 3.8171\n",
            "Discrete prompt: ['encryption', 'kay', 'aunt', 'mum', '##fleet', 'girlfriends', 'primate', 'roommate', 'outright', 'penalties']\n",
            "Epoch 7/50, Loss: 3.5378\n",
            "Discrete prompt: ['encryption', 'kay', 'aunt', 'corsica', '##fleet', 'murdoch', 'primate', 'lesbian', 'vampire', 'penalties']\n",
            "Epoch 8/50, Loss: 3.3284\n",
            "Discrete prompt: ['encryption', 'kay', 'aunt', 'rental', '##fleet', 'nyc', 'primate', 'streamed', 'vampire', 'penalties']\n",
            "Epoch 9/50, Loss: 3.0868\n",
            "Discrete prompt: ['encryption', 'kay', 'aunt', 'rental', '##fleet', 'nyc', 'newcastle', 'streamed', '##uf', 'ringing']\n",
            "Epoch 10/50, Loss: 2.8732\n",
            "Discrete prompt: ['encryption', 'kay', 'savings', 'rental', 'soho', 'nyc', 'newcastle', '##lander', '##uf', 'ringing']\n",
            "Epoch 11/50, Loss: 2.7112\n",
            "Discrete prompt: ['encryption', 'reef', 'savings', 'corsica', 'strait', 'nyc', 'newcastle', '##lander', '##uf', 'ringing']\n",
            "Epoch 12/50, Loss: 2.5197\n",
            "Discrete prompt: ['encryption', 'launcher', 'savings', 'suzanne', 'strait', 'nyc', 'mona', '##lander', '##uf', 'ringing']\n",
            "Epoch 13/50, Loss: 2.3267\n",
            "Discrete prompt: ['encryption', 'launcher', 'haute', 'suzanne', 'strait', 'nyc', 'mona', '##lander', '##uf', 'ringing']\n",
            "Epoch 14/50, Loss: 2.1817\n",
            "Discrete prompt: ['encryption', 'launcher', 'haute', 'kat', 'soho', 'nyc', 'mona', '##lander', '##uf', 'ringing']\n",
            "Epoch 15/50, Loss: 2.0485\n",
            "Discrete prompt: ['encryption', 'launcher', 'haute', 'kat', 'soho', 'nyc', 'mona', '##lander', '##uf', 'ringing']\n",
            "Epoch 16/50, Loss: 1.8851\n",
            "Discrete prompt: ['encryption', 'launcher', 'haute', 'zac', 'soho', 'savings', 'mona', 'patti', '##uf', 'ringing']\n",
            "Epoch 17/50, Loss: 1.7499\n",
            "Discrete prompt: ['throttle', 'launcher', 'irvine', 'zac', 'soho', 'savings', 'mona', 'patti', '##uf', 'ringing']\n",
            "Epoch 18/50, Loss: 1.5895\n",
            "Discrete prompt: ['throttle', 'fiji', 'irvine', 'zac', 'soho', 'savings', 'underwater', 'patti', '##uf', 'ringing']\n",
            "Epoch 19/50, Loss: 1.4681\n",
            "Discrete prompt: ['throttle', 'fiji', 'irvine', 'zac', 'soho', 'savings', 'underwater', 'patti', '##uf', 'ringing']\n",
            "Epoch 20/50, Loss: 1.3803\n",
            "Discrete prompt: ['##ttal', 'fiji', 'irvine', 'zac', 'soho', 'savings', 'underwater', 'patti', 'favourite', 'ringing']\n",
            "Epoch 21/50, Loss: 1.2639\n",
            "Discrete prompt: ['##fleet', 'fiji', 'irvine', 'zac', 'soho', 'savings', 'underwater', 'patti', 'favourite', 'ringing']\n",
            "Epoch 22/50, Loss: 1.1991\n",
            "Discrete prompt: ['##fleet', 'fiji', 'irvine', 'zac', 'soho', 'highs', 'underwater', 'patti', 'difficulty', 'ringing']\n",
            "Epoch 23/50, Loss: 1.1088\n",
            "Discrete prompt: ['##fleet', 'fiji', 'irvine', 'zac', 'soho', 'highs', 'underwater', 'patti', 'difficulty', 'ringing']\n",
            "Epoch 24/50, Loss: 1.0452\n",
            "Discrete prompt: ['##fleet', 'fiji', 'irvine', 'zac', 'soho', 'highs', 'underwater', 'patti', 'difficulty', 'ringing']\n",
            "Epoch 25/50, Loss: 0.9947\n",
            "Discrete prompt: ['##fleet', 'nitrogen', 'irvine', 'zac', 'soho', 'ratings', 'underwater', 'patti', 'difficulty', 'ringing']\n",
            "Epoch 26/50, Loss: 0.9295\n",
            "Discrete prompt: ['##fleet', 'nitrogen', 'irvine', 'zac', 'soho', 'onboard', 'underwater', 'patti', 'difficulty', 'ringing']\n",
            "Epoch 27/50, Loss: 0.8743\n",
            "Discrete prompt: ['##fleet', 'nitrogen', 'resorts', 'zac', 'soho', 'onboard', 'underwater', 'patti', 'difficulty', 'tackled']\n",
            "Epoch 28/50, Loss: 0.8366\n",
            "Discrete prompt: ['##fleet', 'nitrogen', 'resorts', 'zac', 'ken', 'onboard', 'underwater', 'patti', 'difficulty', 'tackled']\n",
            "Epoch 29/50, Loss: 0.7867\n",
            "Discrete prompt: ['##fleet', 'nitrogen', 'resorts', 'zac', 'ken', 'onboard', 'underwater', 'patti', 'difficulty', 'tackled']\n",
            "Epoch 30/50, Loss: 0.7411\n",
            "Discrete prompt: ['##fleet', '##ɒ', 'resorts', 'zac', 'ken', 'onboard', 'underwater', 'patti', 'difficulty', 'tackled']\n",
            "Epoch 31/50, Loss: 0.7177\n",
            "Discrete prompt: ['##fleet', '##ɒ', 'resorts', 'zac', 'ken', 'onboard', 'underwater', 'patti', 'difficulty', 'rebecca']\n",
            "Epoch 32/50, Loss: 0.6704\n",
            "Discrete prompt: ['##fleet', '##ɒ', 'resorts', 'zac', 'ken', 'onboard', 'underwater', 'patti', 'difficulty', 'rebecca']\n",
            "Epoch 33/50, Loss: 0.6470\n",
            "Discrete prompt: ['##fleet', '##ɒ', 'resorts', 'zac', 'ken', 'onboard', 'underwater', 'patti', 'difficulty', 'rebecca']\n",
            "Epoch 34/50, Loss: 0.6066\n",
            "Discrete prompt: ['##fleet', '##ɒ', 'dans', 'zac', 'ken', 'onboard', 'onboard', 'patti', 'difficulty', 'rebecca']\n",
            "Epoch 35/50, Loss: 0.5811\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'zac', 'ken', 'onboard', 'onboard', 'patti', 'difficulty', 'rebecca']\n",
            "Epoch 36/50, Loss: 0.5581\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'zac', 'ken', 'onboard', 'onboard', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 37/50, Loss: 0.5230\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', '##ann', 'ken', 'onboard', 'onboard', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 38/50, Loss: 0.4976\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', '##ann', 'ken', 'onboard', 'onboard', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 39/50, Loss: 0.4765\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', '##ann', 'ken', 'onboard', 'onboard', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 40/50, Loss: 0.4577\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'onboard', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 41/50, Loss: 0.4297\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'onboard', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 42/50, Loss: 0.4210\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'onboard', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 43/50, Loss: 0.4121\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 44/50, Loss: 0.3854\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 45/50, Loss: 0.3777\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 46/50, Loss: 0.3628\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 47/50, Loss: 0.3576\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 48/50, Loss: 0.3455\n",
            "Discrete prompt: ['##nall', '##ɒ', 'dans', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'difficulty', 'rebecca']\n",
            "Epoch 49/50, Loss: 0.3319\n",
            "Discrete prompt: ['##nall', '##ɒ', 'bing', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'whales', 'rebecca']\n",
            "Epoch 50/50, Loss: 0.3205\n",
            "Discrete prompt: ['##nall', '##ɒ', 'bing', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'whales', 'rebecca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# P1\n",
        "\n",
        "'''\n",
        "Epoch 50/50, Loss: 0.3205, lr = 4e-3\n",
        "Discrete prompt: ['##nall', '##ɒ', 'bing', 'beloved', 'ken', 'detectives', 'conan', '##lander', 'whales', 'rebecca']\n",
        "'''\n",
        "\n",
        "top_tokens = torch.argmax(soft_prompt, dim=-1)  # [prompt_length]\n",
        "tokens = top_tokens.unsqueeze(0)  # shape: [1, prompt_length]\n",
        "\n",
        "# Get embedding layer\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(tokens).squeeze(0)  # shape: [prompt_length, hidden_size]\n",
        "\n",
        "# Save to file\n",
        "torch.save(embeddings, \"simple_50_embedding.pt\")"
      ],
      "metadata": {
        "id": "YTU84B6ZjLxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# P2\n",
        "\n",
        "'''\n",
        "Epoch 50/50, Loss: 5.7666, Fluency:5.5766,  Bias:0.1900,  lr = 5e-3\n",
        "Discrete prompt: ['〈', '\"', 'code', 'ken', 'batter', 'beautifully', 'pop', 'royal', 'odi', 'pillow']\n",
        "\n",
        "'''\n",
        "top_tokens = torch.argmax(soft_prompt, dim=-1)  # [prompt_length]\n",
        "tokens = top_tokens.unsqueeze(0)  # shape: [1, prompt_length]\n",
        "\n",
        "# Get embedding layer\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(tokens).squeeze(0)  # shape: [prompt_length, hidden_size]\n",
        "\n",
        "# Save to file (e.g., .pt or .npy)\n",
        "torch.save(embeddings, \"50_causal_embedding.pt\")"
      ],
      "metadata": {
        "id": "lQIA3R3ki4Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similrity Constraint with Bertscore"
      ],
      "metadata": {
        "id": "z0IiavSso83A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bertscore_constraint(soft_prompt_logits, ref_text, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Compute differentiable similarity loss between soft prompt and a reference hard prompt using BERTScore.\n",
        "\n",
        "    Args:\n",
        "        soft_prompt_logits: (T, V) unnormalized token logits\n",
        "        ref_text (str): reference string prompt\n",
        "        tokenizer: HuggingFace tokenizer (matching vocab of soft prompt)\n",
        "        bert_model: HuggingFace BERT model (output_hidden_states=True)\n",
        "\n",
        "    Returns:\n",
        "        loss: 1 - BERTScore (higher = less similar)\n",
        "    \"\"\"\n",
        "\n",
        "    device = soft_prompt_logits.device\n",
        "    soft_probs = F.softmax(soft_prompt_logits, dim=-1)  # (T, V)\n",
        "    T, vocab_size = soft_probs.size()\n",
        "\n",
        "    # 1. Embed soft prompt\n",
        "    emb_mat = model.get_input_embeddings().weight  # (V, D)\n",
        "    soft_embeds = soft_probs @ emb_mat  # (T, D)\n",
        "\n",
        "    # Add batch dim for BERT: (1, T, D)\n",
        "    soft_embeds = soft_embeds.unsqueeze(0)\n",
        "\n",
        "    # 2. Tokenize and encode the reference prompt\n",
        "    ref_tokens = tokenizer(ref_text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "    ref_output = model(**ref_tokens, output_hidden_states=True)\n",
        "    ref_embed = ref_output.hidden_states[-1].squeeze(0)  # (T_ref, D)\n",
        "\n",
        "    # 3. Encode soft prompt using BERT\n",
        "    soft_output = model(inputs_embeds=soft_embeds, output_hidden_states=True)\n",
        "    soft_embed = soft_output.hidden_states[-1].squeeze(0)  # (T_soft, D)\n",
        "\n",
        "    # 4. Normalize embeddings\n",
        "    soft_embed = F.normalize(soft_embed, p=2, dim=1)  # (T_soft, D)\n",
        "    ref_embed  = F.normalize(ref_embed,  p=2, dim=1)  # (T_ref, D)\n",
        "\n",
        "    # 5. Compute cosine similarities: (T_ref, T_soft)\n",
        "    sim_matrix = ref_embed @ soft_embed.T  # cosine similarity\n",
        "\n",
        "    # 6. For each ref token, take best matching soft token\n",
        "    max_sim = sim_matrix.max(dim=1)[0]  # (T_ref,)\n",
        "\n",
        "    # 7. Final similarity score and loss\n",
        "    bertscore = max_sim.mean()\n",
        "    sim_loss = 1.0 - bertscore\n",
        "\n",
        "    return sim_loss\n"
      ],
      "metadata": {
        "id": "xkHhwYQONUf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ref_prompt_constructer(EPOCHS,optimizer,train_loader,soft_prompt,prompt_length,ref_text):\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "      total_loss = 0\n",
        "      total_main_loss = 0\n",
        "      total_fluency_loss = 0\n",
        "      total_sim_loss = 0\n",
        "      #total_fluency_GPT = 0\n",
        "\n",
        "      total_causal_fluency = 0\n",
        "\n",
        "      for batch in train_loader:\n",
        "          '''\n",
        "          # Debug: Print device info for all tensors\n",
        "          print(\"Checking devices:\")\n",
        "          for k, v in batch.items():\n",
        "              if isinstance(v, torch.Tensor):\n",
        "                  print(f\"{k}: {v.device}\")\n",
        "              elif isinstance(v, list) and len(v) > 0 and isinstance(v[0], torch.Tensor):\n",
        "                  print(f\"{k}: {v[0].device} (list of tensors)\")\n",
        "          '''\n",
        "\n",
        "          model.zero_grad()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Compute soft prompt embedding\n",
        "          input_ids_m = batch[\"input_ids_male\"]\n",
        "          input_ids_f = batch[\"input_ids_female\"]\n",
        "          attn_mask_m = batch[\"attention_mask_male\"]\n",
        "          attn_mask_f = batch[\"attention_mask_female\"]\n",
        "          mask_idx_m = batch[\"mask_token_idx_male\"] + prompt_length\n",
        "          mask_idx_f = batch[\"mask_token_idx_female\"] + prompt_length\n",
        "          output_indices = batch[\"output_indices\"]\n",
        "\n",
        "          # Embed input + prefix\n",
        "          inputs_embeds_m = model.get_input_embeddings()(input_ids_m)\n",
        "          inputs_embeds_f = model.get_input_embeddings()(input_ids_f)\n",
        "\n",
        "          embedded_prompt = get_embedded_prompt(soft_prompt, model)\n",
        "\n",
        "          # Extend attention masks first\n",
        "          prompt_length = embedded_prompt.size(0)\n",
        "          attn_mask_m = torch.cat([\n",
        "              torch.ones((attn_mask_m.size(0), prompt_length),\n",
        "                        dtype=attn_mask_m.dtype,\n",
        "                        device=device),\n",
        "              attn_mask_m\n",
        "          ], dim=1)\n",
        "          attn_mask_f = torch.cat([\n",
        "              torch.ones((attn_mask_f.size(0), prompt_length),\n",
        "                        dtype=attn_mask_f.dtype,\n",
        "                        device=device),\n",
        "              attn_mask_f\n",
        "          ], dim=1)\n",
        "\n",
        "          inputs_embeds_m = torch.cat([embedded_prompt.unsqueeze(0).expand(inputs_embeds_m.size(0), -1, -1), inputs_embeds_m], dim=1)\n",
        "          inputs_embeds_f = torch.cat([embedded_prompt.unsqueeze(0).expand(inputs_embeds_f.size(0), -1, -1), inputs_embeds_f], dim=1)\n",
        "\n",
        "          outputs_m = model(attention_mask=attn_mask_m, inputs_embeds=inputs_embeds_m)\n",
        "          outputs_f = model(attention_mask=attn_mask_f, inputs_embeds=inputs_embeds_f)\n",
        "\n",
        "          logits_m = outputs_m.logits[torch.arange(len(mask_idx_m)), mask_idx_m]\n",
        "          logits_f = outputs_f.logits[torch.arange(len(mask_idx_f)), mask_idx_f]\n",
        "\n",
        "          loss_main = loss_equal_valid_options_mask_logits(logits_m, logits_f, output_indices)\n",
        "\n",
        "          # Fluency\n",
        "\n",
        "          fluency_loss = causal_fluency(soft_prompt,ref_model)\n",
        "          lambda_fluency = 0.025  # Tune this weight\n",
        "\n",
        "\n",
        "          # Similarity constraint\n",
        "\n",
        "          loss_similar = bertscore_constraint(soft_prompt, ref_text, tokenizer, model)\n",
        "          lmb_sim = 0.5 # Tune this weight\n",
        "\n",
        "          loss = loss_main + lambda_fluency * fluency_loss + lmb_sim * loss_similar\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_main_loss += loss_main.item()\n",
        "          total_fluency_loss += fluency_loss.item()\n",
        "          total_sim_loss += loss_similar.item()\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}, Fluency:{(lambda_fluency * total_fluency_loss):.4f},  Bias:{total_main_loss:.4f}, Similar:{lmb_sim *total_sim_loss:.4f} \")\n",
        "\n",
        "      top_tokens = torch.argmax(soft_prompt, dim=-1)  # [prompt_length]\n",
        "      prompt_tokens=top_tokens.tolist()\n",
        "      discrete_prompt = tokenizer.convert_ids_to_tokens(prompt_tokens)\n",
        "      print(\"Discrete prompt:\", discrete_prompt)\n",
        "\n",
        "  return soft_prompt"
      ],
      "metadata": {
        "id": "djTk7bRtx6Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_text = \"fair gender equal profession male female unbiased\"\n",
        "ref_ids = tokenizer.encode(ref_text, add_special_tokens=False)\n",
        "print(\"Token count:\", len(ref_ids))\n",
        "prompt_length = len(ref_ids)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "soft_prompt_init = torch.randn(prompt_length, vocab_size, device=device) * 0.01\n",
        "'''\n",
        "# One-hot encode first tokens\n",
        "for i, token_id in enumerate(init_ids):\n",
        "    soft_prompt_init[i, token_id] = 2  # logit to push toward that token\n",
        "'''\n",
        "soft_prompt = nn.Parameter(soft_prompt_init, requires_grad=True)\n",
        "optimizer = CustomAdam([soft_prompt], lr=4e-3)\n",
        "EPOCHS = 100\n",
        "soft_prompt = ref_prompt_constructer(EPOCHS,optimizer,train_loader,soft_prompt,prompt_length, ref_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y7-DHbIzRX9",
        "outputId": "a453b066-d34a-46fd-e37e-70b81a3dbd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 9\n",
            "Epoch 1/100, Loss: 12.4026, Fluency:3.4067,  Bias:6.1607, Similar:2.8352 \n",
            "Discrete prompt: ['eyre', 'karel', '##eman', '##و', 'spelling', 'papua', 'feat', 'number', 'strokes']\n",
            "Epoch 2/100, Loss: 11.5489, Fluency:3.4094,  Bias:5.4720, Similar:2.6675 \n",
            "Discrete prompt: ['game', 'disqualification', '##wt', 'charting', 'spelling', 'founded', '##urt', 'bun', '##virus']\n",
            "Epoch 3/100, Loss: 10.8653, Fluency:3.4230,  Bias:4.8823, Similar:2.5600 \n",
            "Discrete prompt: ['game', 'consent', '##wt', 'charting', '##hat', 'acknowledged', 'bitten', 'bun', 'wolves']\n",
            "Epoch 4/100, Loss: 10.3429, Fluency:3.4132,  Bias:4.4799, Similar:2.4498 \n",
            "Discrete prompt: ['bad', 'consent', 'kidd', 'charting', '##kal', 'slits', 'predicted', 'bay', 'wolves']\n",
            "Epoch 5/100, Loss: 9.8363, Fluency:3.3775,  Bias:4.1130, Similar:2.3459 \n",
            "Discrete prompt: ['bad', 'ant', 'kidd', 'ncaa', '##gh', 'slits', 'lest', 'bay', '##ctic']\n",
            "Epoch 6/100, Loss: 9.3382, Fluency:3.3543,  Bias:3.7408, Similar:2.2431 \n",
            "Discrete prompt: ['bad', 'ant', 'chow', 'ncaa', '##gh', 'haute', 'supposed', 'haute', 'vamp']\n",
            "Epoch 7/100, Loss: 8.9583, Fluency:3.3642,  Bias:3.4681, Similar:2.1260 \n",
            "Discrete prompt: ['bad', 'ken', 'chow', 'wakefield', '##gh', 'haute', 'supposed', 'teddy', 'vamp']\n",
            "Epoch 8/100, Loss: 8.6315, Fluency:3.3775,  Bias:3.2381, Similar:2.0159 \n",
            "Discrete prompt: ['boogie', '##raz', 'chow', 'wakefield', '##gh', 'haute', 'supposed', 'hurricane', 'vampire']\n",
            "Epoch 9/100, Loss: 8.2972, Fluency:3.3328,  Bias:3.0203, Similar:1.9440 \n",
            "Discrete prompt: ['boogie', '##raz', 'seam', 'wakefield', 'pornography', 'haute', 'enabling', '##cano', 'vampire']\n",
            "Epoch 10/100, Loss: 8.0655, Fluency:3.3385,  Bias:2.8359, Similar:1.8911 \n",
            "Discrete prompt: ['newt', '##raz', 'seam', 'wakefield', 'pornography', 'durable', 'enabling', '##cano', 'vampire']\n",
            "Epoch 11/100, Loss: 7.8135, Fluency:3.3109,  Bias:2.6525, Similar:1.8500 \n",
            "Discrete prompt: ['density', 'eireann', 'cock', 'wakefield', 'pornography', 'durable', 'enabling', '##cano', 'vampire']\n",
            "Epoch 12/100, Loss: 7.5378, Fluency:3.2857,  Bias:2.4382, Similar:1.8140 \n",
            "Discrete prompt: ['density', 'eireann', 'cock', 'wakefield', 'pornography', 'durable', 'enabling', '##cano', 'genius']\n",
            "Epoch 13/100, Loss: 7.3196, Fluency:3.3025,  Bias:2.2360, Similar:1.7810 \n",
            "Discrete prompt: ['density', 'eireann', 'cock', 'wakefield', 'pornography', 'durable', 'enabling', '##cano', 'genius']\n",
            "Epoch 14/100, Loss: 7.1085, Fluency:3.2996,  Bias:2.0557, Similar:1.7532 \n",
            "Discrete prompt: ['density', 'eireann', 'cock', 'wakefield', 'wakefield', 'durable', 'enabling', 'gambling', 'lady']\n",
            "Epoch 15/100, Loss: 6.8574, Fluency:3.2774,  Bias:1.8527, Similar:1.7273 \n",
            "Discrete prompt: ['density', 'eireann', 'cock', 'wakefield', 'wakefield', 'durable', 'enabling', 'gambling', 'lady']\n",
            "Epoch 16/100, Loss: 6.6452, Fluency:3.2726,  Bias:1.6720, Similar:1.7006 \n",
            "Discrete prompt: ['density', 'eireann', 'cock', 'wakefield', 'wakefield', 'durable', 'enabling', 'hacking', 'lady']\n",
            "Epoch 17/100, Loss: 6.4282, Fluency:3.2614,  Bias:1.4948, Similar:1.6720 \n",
            "Discrete prompt: ['density', 'eireann', 'ham', 'wakefield', 'wakefield', 'durable', 'verified', 'hacking', 'lady']\n",
            "Epoch 18/100, Loss: 6.3080, Fluency:3.2768,  Bias:1.3850, Similar:1.6462 \n",
            "Discrete prompt: ['density', 'eireann', 'ham', 'wakefield', 'wakefield', 'durable', 'verified', 'hacking', 'lady']\n",
            "Epoch 19/100, Loss: 6.1519, Fluency:3.2684,  Bias:1.2598, Similar:1.6237 \n",
            "Discrete prompt: ['density', 'eireann', 'ham', 'wakefield', 'wakefield', 'durable', 'verified', 'hacking', 'lady']\n",
            "Epoch 20/100, Loss: 5.9987, Fluency:3.2445,  Bias:1.1510, Similar:1.6033 \n",
            "Discrete prompt: ['density', 'eireann', 'sheridan', 'wakefield', 'wakefield', 'durable', 'verified', 'hacking', 'lady']\n",
            "Epoch 21/100, Loss: 5.8730, Fluency:3.2404,  Bias:1.0484, Similar:1.5842 \n",
            "Discrete prompt: ['pale', 'eireann', 'sheridan', 'simi', 'chow', 'durable', 'haute', 'hacking', 'lady']\n",
            "Epoch 22/100, Loss: 5.7859, Fluency:3.2406,  Bias:0.9818, Similar:1.5635 \n",
            "Discrete prompt: ['pale', 'eireann', 'sheridan', 'simi', 'zoe', 'domestic', 'haute', 'hacking', 'lady']\n",
            "Epoch 23/100, Loss: 5.6771, Fluency:3.2314,  Bias:0.9013, Similar:1.5444 \n",
            "Discrete prompt: ['pale', 'eireann', 'sheridan', 'simi', 'zoe', 'registry', 'haute', 'hacking', 'lady']\n",
            "Epoch 24/100, Loss: 5.5885, Fluency:3.2110,  Bias:0.8513, Similar:1.5262 \n",
            "Discrete prompt: ['pale', 'eireann', 'sheridan', 'simi', 'chow', 'registry', 'haute', 'hacking', 'lady']\n",
            "Epoch 25/100, Loss: 5.5188, Fluency:3.2124,  Bias:0.7997, Similar:1.5068 \n",
            "Discrete prompt: ['pale', 'eireann', 'sheridan', 'simi', 'chow', 'registry', 'haute', 'hacking', 'lady']\n",
            "Epoch 26/100, Loss: 5.4288, Fluency:3.1991,  Bias:0.7407, Similar:1.4889 \n",
            "Discrete prompt: ['pale', 'eireann', 'sheridan', 'simi', 'chow', 'registry', 'haute', 'pity', 'lady']\n",
            "Epoch 27/100, Loss: 5.3641, Fluency:3.2020,  Bias:0.6910, Similar:1.4712 \n",
            "Discrete prompt: ['artificial', 'eireann', 'sheridan', 'simi', 'chow', 'registry', 'haute', 'pity', 'lady']\n",
            "Epoch 28/100, Loss: 5.3168, Fluency:3.2002,  Bias:0.6637, Similar:1.4530 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', 'chow', 'registry', 'haute', 'pity', 'lady']\n",
            "Epoch 29/100, Loss: 5.2534, Fluency:3.1871,  Bias:0.6308, Similar:1.4355 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', 'chow', 'registry', 'haute', 'pity', 'lady']\n",
            "Epoch 30/100, Loss: 5.1912, Fluency:3.1729,  Bias:0.6004, Similar:1.4178 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', 'chow', 'aide', 'whose', 'pity', 'lady']\n",
            "Epoch 31/100, Loss: 5.1109, Fluency:3.1502,  Bias:0.5598, Similar:1.4009 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', 'chow', 'aide', 'whose', 'pity', 'lady']\n",
            "Epoch 32/100, Loss: 5.0903, Fluency:3.1516,  Bias:0.5547, Similar:1.3840 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', 'chow', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 33/100, Loss: 5.0303, Fluency:3.1578,  Bias:0.5052, Similar:1.3673 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', 'chow', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 34/100, Loss: 4.9701, Fluency:3.1208,  Bias:0.4987, Similar:1.3506 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 35/100, Loss: 4.9236, Fluency:3.1262,  Bias:0.4632, Similar:1.3343 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 36/100, Loss: 4.8724, Fluency:3.1073,  Bias:0.4466, Similar:1.3185 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 37/100, Loss: 4.8268, Fluency:3.1029,  Bias:0.4217, Similar:1.3022 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 38/100, Loss: 4.8109, Fluency:3.1151,  Bias:0.4098, Similar:1.2861 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 39/100, Loss: 4.7390, Fluency:3.0666,  Bias:0.4017, Similar:1.2707 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 40/100, Loss: 4.6908, Fluency:3.0633,  Bias:0.3717, Similar:1.2558 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 41/100, Loss: 4.6483, Fluency:3.0482,  Bias:0.3595, Similar:1.2406 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tame', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 42/100, Loss: 4.6505, Fluency:3.0708,  Bias:0.3536, Similar:1.2261 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tram', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 43/100, Loss: 4.6059, Fluency:3.0582,  Bias:0.3355, Similar:1.2122 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tram', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 44/100, Loss: 4.5638, Fluency:3.0431,  Bias:0.3222, Similar:1.1985 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tram', 'simi', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 45/100, Loss: 4.4982, Fluency:3.0066,  Bias:0.3066, Similar:1.1850 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tram', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 46/100, Loss: 4.4811, Fluency:3.0127,  Bias:0.2962, Similar:1.1721 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tram', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 47/100, Loss: 4.4557, Fluency:3.0056,  Bias:0.2907, Similar:1.1594 \n",
            "Discrete prompt: ['artificial', 'eireann', 'tram', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 48/100, Loss: 4.4238, Fluency:2.9986,  Bias:0.2782, Similar:1.1469 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 49/100, Loss: 4.3818, Fluency:2.9719,  Bias:0.2749, Similar:1.1349 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 50/100, Loss: 4.3466, Fluency:2.9579,  Bias:0.2654, Similar:1.1233 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 51/100, Loss: 4.3323, Fluency:2.9570,  Bias:0.2636, Similar:1.1117 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 52/100, Loss: 4.2966, Fluency:2.9393,  Bias:0.2570, Similar:1.1003 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 53/100, Loss: 4.2705, Fluency:2.9308,  Bias:0.2504, Similar:1.0893 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 54/100, Loss: 4.2269, Fluency:2.8995,  Bias:0.2486, Similar:1.0787 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 55/100, Loss: 4.2314, Fluency:2.9231,  Bias:0.2398, Similar:1.0685 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 56/100, Loss: 4.2026, Fluency:2.9111,  Bias:0.2326, Similar:1.0590 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'mascot', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 57/100, Loss: 4.1749, Fluency:2.8938,  Bias:0.2314, Similar:1.0496 \n",
            "Discrete prompt: ['artificial', 'eireann', 'rowe', 'greenwich', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 58/100, Loss: 4.1542, Fluency:2.8920,  Bias:0.2224, Similar:1.0399 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'greenwich', '’', 'aide', 'whose', 'pity', 'student']\n",
            "Epoch 59/100, Loss: 4.1175, Fluency:2.8663,  Bias:0.2210, Similar:1.0302 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'greenwich', '’', '(', 'whose', '##wo', 'student']\n",
            "Epoch 60/100, Loss: 4.0788, Fluency:2.8440,  Bias:0.2139, Similar:1.0209 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'greenwich', '’', '(', 'whose', '##wo', 'student']\n",
            "Epoch 61/100, Loss: 4.0597, Fluency:2.8306,  Bias:0.2175, Similar:1.0116 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'greenwich', '’', '(', 'whose', '##wo', 'ceremony']\n",
            "Epoch 62/100, Loss: 4.0498, Fluency:2.8318,  Bias:0.2157, Similar:1.0023 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'greenwich', '’', '(', 'whose', '##wo', 'ceremony']\n",
            "Epoch 63/100, Loss: 4.0427, Fluency:2.8415,  Bias:0.2081, Similar:0.9931 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'greenwich', '’', '(', 'whose', '##wo', 'ceremony']\n",
            "Epoch 64/100, Loss: 4.0026, Fluency:2.8132,  Bias:0.2050, Similar:0.9845 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'mascot', '’', '(', 'whose', '##wo', 'dictionary']\n",
            "Epoch 65/100, Loss: 3.9906, Fluency:2.8113,  Bias:0.2034, Similar:0.9759 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'greenwich', '’', '(', 'whose', 'ship', 'dictionary']\n",
            "Epoch 66/100, Loss: 3.9620, Fluency:2.7945,  Bias:0.1998, Similar:0.9678 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'television', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 67/100, Loss: 3.9484, Fluency:2.7884,  Bias:0.2007, Similar:0.9593 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'television', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 68/100, Loss: 3.9297, Fluency:2.7845,  Bias:0.1937, Similar:0.9515 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'television', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 69/100, Loss: 3.9091, Fluency:2.7738,  Bias:0.1909, Similar:0.9444 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'television', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 70/100, Loss: 3.8734, Fluency:2.7469,  Bias:0.1888, Similar:0.9377 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'television', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 71/100, Loss: 3.8704, Fluency:2.7525,  Bias:0.1870, Similar:0.9310 \n",
            "Discrete prompt: ['artificial', '@', 'rowe', 'television', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 72/100, Loss: 3.8495, Fluency:2.7407,  Bias:0.1841, Similar:0.9246 \n",
            "Discrete prompt: ['artificial', '@', ']', 'television', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 73/100, Loss: 3.8375, Fluency:2.7342,  Bias:0.1849, Similar:0.9184 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 74/100, Loss: 3.8143, Fluency:2.7189,  Bias:0.1832, Similar:0.9122 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 75/100, Loss: 3.8223, Fluency:2.7299,  Bias:0.1857, Similar:0.9067 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 76/100, Loss: 3.7840, Fluency:2.7028,  Bias:0.1799, Similar:0.9013 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 77/100, Loss: 3.7611, Fluency:2.6885,  Bias:0.1767, Similar:0.8959 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 78/100, Loss: 3.7664, Fluency:2.6982,  Bias:0.1777, Similar:0.8906 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 79/100, Loss: 3.7555, Fluency:2.6937,  Bias:0.1762, Similar:0.8856 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 80/100, Loss: 3.7338, Fluency:2.6797,  Bias:0.1727, Similar:0.8813 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'whose', 'ship', '##lov']\n",
            "Epoch 81/100, Loss: 3.7208, Fluency:2.6683,  Bias:0.1754, Similar:0.8771 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'potato', 'ship', '##lov']\n",
            "Epoch 82/100, Loss: 3.7353, Fluency:2.6895,  Bias:0.1730, Similar:0.8728 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', '(', 'potato', '##wai', '##lov']\n",
            "Epoch 83/100, Loss: 3.7154, Fluency:2.6693,  Bias:0.1775, Similar:0.8686 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', 'paralympic', 'potato', '##wai', '##lov']\n",
            "Epoch 84/100, Loss: 3.7068, Fluency:2.6715,  Bias:0.1704, Similar:0.8650 \n",
            "Discrete prompt: ['artificial', '@', ']', '[', '’', 'paralympic', 'potato', '##wai', '##lov']\n",
            "Epoch 85/100, Loss: 3.6925, Fluency:2.6624,  Bias:0.1685, Similar:0.8616 \n",
            "Discrete prompt: ['artificial', '@', ']', '<', '’', 'paralympic', 'whose', '##wai', '##lov']\n",
            "Epoch 86/100, Loss: 3.6805, Fluency:2.6531,  Bias:0.1692, Similar:0.8582 \n",
            "Discrete prompt: ['artificial', '@', ']', '<', '’', 'paralympic', 'whose', '##wai', '##lov']\n",
            "Epoch 87/100, Loss: 3.6487, Fluency:2.6271,  Bias:0.1665, Similar:0.8551 \n",
            "Discrete prompt: ['artificial', '@', ']', '<', '’', 'paralympic', 'potato', '##wai', '##lov']\n",
            "Epoch 88/100, Loss: 3.6613, Fluency:2.6399,  Bias:0.1693, Similar:0.8520 \n",
            "Discrete prompt: ['artificial', '@', ']', '<', '’', 'paralympic', 'whose', '##wai', '##lov']\n",
            "Epoch 89/100, Loss: 3.6528, Fluency:2.6315,  Bias:0.1716, Similar:0.8497 \n",
            "Discrete prompt: ['artificial', '@', ']', 'turtles', '–', 'paralympic', 'whose', '##wai', '##lov']\n",
            "Epoch 90/100, Loss: 3.6447, Fluency:2.6280,  Bias:0.1697, Similar:0.8470 \n",
            "Discrete prompt: ['artificial', '@', ']', 'turtles', '–', 'paralympic', 'potato', '##wai', '##lov']\n",
            "Epoch 91/100, Loss: 3.6222, Fluency:2.6117,  Bias:0.1660, Similar:0.8445 \n",
            "Discrete prompt: ['artificial', '@', ']', '<', '–', 'pork', 'potato', '##wai', '##lov']\n",
            "Epoch 92/100, Loss: 3.6226, Fluency:2.6129,  Bias:0.1678, Similar:0.8419 \n",
            "Discrete prompt: ['artificial', '@', ']', '<', '–', 'pork', 'potato', '##wai', '##lov']\n",
            "Epoch 93/100, Loss: 3.6145, Fluency:2.6093,  Bias:0.1658, Similar:0.8395 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'potato', 'dance', '##lov']\n",
            "Epoch 94/100, Loss: 3.6137, Fluency:2.6103,  Bias:0.1661, Similar:0.8373 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'potato', 'dance', '##lov']\n",
            "Epoch 95/100, Loss: 3.5920, Fluency:2.5911,  Bias:0.1654, Similar:0.8356 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'whose', 'dance', '##lov']\n",
            "Epoch 96/100, Loss: 3.5856, Fluency:2.5877,  Bias:0.1642, Similar:0.8338 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'whose', 'dance', 'student']\n",
            "Epoch 97/100, Loss: 3.5947, Fluency:2.5989,  Bias:0.1636, Similar:0.8323 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'whose', 'dance', 'student']\n",
            "Epoch 98/100, Loss: 3.5743, Fluency:2.5800,  Bias:0.1637, Similar:0.8306 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'whose', 'dance', 'student']\n",
            "Epoch 99/100, Loss: 3.5497, Fluency:2.5589,  Bias:0.1622, Similar:0.8286 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'whose', 'dance', 'student']\n",
            "Epoch 100/100, Loss: 3.5408, Fluency:2.5528,  Bias:0.1616, Similar:0.8264 \n",
            "Discrete prompt: ['artificial', '@', 'wild', '<', '–', 'pork', 'whose', 'dance', 'student']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# P3\n",
        "\n",
        "discrete_prompt = ['artificial', '@', 'wild', '<', '–', 'pork', 'whose', 'dance', 'student']\n",
        "prompt_tokens = tokenizer.convert_tokens_to_ids(discrete_prompt)\n",
        "top_tokens = torch.tensor(prompt_tokens)\n",
        "\n",
        "tokens = top_tokens.unsqueeze(0)  # shape: [1, prompt_length]\n",
        "tokens = tokens.to(device)\n",
        "# Get embedding layer\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(tokens).squeeze(0)  # shape: [prompt_length, hidden_size]\n",
        "\n",
        "# Save to file (e.g., .pt or .npy)\n",
        "torch.save(embeddings, \"constricted_embedding.pt\")"
      ],
      "metadata": {
        "id": "pPf5lz8lWz6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional experiments (Fluency)"
      ],
      "metadata": {
        "id": "tnrJ2feXoCQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 70 epoch s, lr 5e-3 , causal fluency\n",
        "discrete_prompt = ['##tang', '##s', 'wild', 'baskets', 'spaceship', 'rated', ',', 'loved', '(', 'pillow']\n",
        "prompt_tokens = tokenizer.convert_tokens_to_ids(discrete_prompt)\n",
        "top_tokens = torch.tensor(prompt_tokens)\n",
        "\n",
        "tokens = top_tokens.unsqueeze(0)  # shape: [1, prompt_length]\n",
        "tokens = tokens.to(device)\n",
        "# Get embedding layer\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(tokens).squeeze(0)  # shape: [prompt_length, hidden_size]\n",
        "\n",
        "# Save to file (e.g., .pt or .npy)\n",
        "torch.save(embeddings, \"70_causal_embedding.pt\")"
      ],
      "metadata": {
        "id": "pT_JdYCBQoyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_length = 5\n",
        "vocab_size = tokenizer.vocab_size\n",
        "soft_prompt = nn.Parameter(torch.zeros(prompt_length, vocab_size,device=device), requires_grad=True)\n",
        "optimizer = CustomAdam([soft_prompt], lr=5e-3)\n",
        "EPOCHS = 50\n",
        "\n",
        "soft_prompt = prompt_constructer(EPOCHS,optimizer,train_loader,soft_prompt,prompt_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjZOOT1C93uz",
        "outputId": "1ee39f1f-b9d9-4278-dd14-6112685b8f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 13.5944, Fluency:6.8320,  Bias:6.7624\n",
            "Discrete prompt: ['alba', '##back', 'possession', 'holocaust', 'games']\n",
            "Epoch 2/50, Loss: 13.1401, Fluency:6.7987,  Bias:6.3414\n",
            "Discrete prompt: ['##tal', '##race', 'beforehand', 'bombed', 'games']\n",
            "Epoch 3/50, Loss: 12.6061, Fluency:6.6330,  Bias:5.9732\n",
            "Discrete prompt: ['##tal', '##hum', 'beforehand', 'cancel', 'csi']\n",
            "Epoch 4/50, Loss: 12.3433, Fluency:6.5791,  Bias:5.7642\n",
            "Discrete prompt: ['##tal', '##hum', 'mirage', 'strip', 'csi']\n",
            "Epoch 5/50, Loss: 12.2494, Fluency:6.6052,  Bias:5.6441\n",
            "Discrete prompt: ['##tal', '##itical', 'seam', 'strip', 'csi']\n",
            "Epoch 6/50, Loss: 11.9167, Fluency:6.4830,  Bias:5.4337\n",
            "Discrete prompt: ['sul', '##itical', 'graphic', 'whale', 'csi']\n",
            "Epoch 7/50, Loss: 11.6416, Fluency:6.4252,  Bias:5.2164\n",
            "Discrete prompt: ['sul', '##itical', 'graphic', 'whale', 'csi']\n",
            "Epoch 8/50, Loss: 11.3467, Fluency:6.3613,  Bias:4.9854\n",
            "Discrete prompt: ['xinjiang', '##itical', 'graphic', 'whale', 'ecstasy']\n",
            "Epoch 9/50, Loss: 11.0749, Fluency:6.3096,  Bias:4.7653\n",
            "Discrete prompt: ['odi', '##itical', 'graphic', 'whale', 'ecstasy']\n",
            "Epoch 10/50, Loss: 10.8199, Fluency:6.2937,  Bias:4.5262\n",
            "Discrete prompt: ['odi', '##itical', 'graphic', 'whale', 'ecstasy']\n",
            "Epoch 11/50, Loss: 10.5387, Fluency:6.2608,  Bias:4.2779\n",
            "Discrete prompt: ['##kt', '##itical', '##tlement', 'whale', 'ecstasy']\n",
            "Epoch 12/50, Loss: 10.3754, Fluency:6.2710,  Bias:4.1045\n",
            "Discrete prompt: ['sul', '##itical', '##tlement', 'stunt', 'ecstasy']\n",
            "Epoch 13/50, Loss: 10.0888, Fluency:6.1880,  Bias:3.9008\n",
            "Discrete prompt: ['sul', 'offshore', '##tlement', 'stunt', 'ecstasy']\n",
            "Epoch 14/50, Loss: 9.8602, Fluency:6.1337,  Bias:3.7265\n",
            "Discrete prompt: ['sul', 'offshore', '##tlement', 'stunt', 'seductive']\n",
            "Epoch 15/50, Loss: 9.5810, Fluency:6.0410,  Bias:3.5400\n",
            "Discrete prompt: ['satisfactory', '##nable', '##tlement', 'stunt', 'seductive']\n",
            "Epoch 16/50, Loss: 9.4540, Fluency:6.0887,  Bias:3.3652\n",
            "Discrete prompt: ['satisfactory', '##nable', '##tlement', 'stunt', 'seductive']\n",
            "Epoch 17/50, Loss: 9.2384, Fluency:6.0416,  Bias:3.1968\n",
            "Discrete prompt: ['satisfactory', '##tory', '##tlement', 'stunt', 'seductive']\n",
            "Epoch 18/50, Loss: 9.0182, Fluency:5.9993,  Bias:3.0189\n",
            "Discrete prompt: ['sul', '##tory', '##tlement', 'stunt', 'seductive']\n",
            "Epoch 19/50, Loss: 8.7660, Fluency:5.9194,  Bias:2.8466\n",
            "Discrete prompt: ['sul', '##tory', '##tlement', 'stunt', 'seductive']\n",
            "Epoch 20/50, Loss: 8.5603, Fluency:5.8623,  Bias:2.6980\n",
            "Discrete prompt: ['sul', '##tory', '##tlement', 'stunt', 'sexually']\n",
            "Epoch 21/50, Loss: 8.3972, Fluency:5.7895,  Bias:2.6077\n",
            "Discrete prompt: ['sul', '##tory', 'illustrations', 'stunt', 'sexually']\n",
            "Epoch 22/50, Loss: 8.1939, Fluency:5.7591,  Bias:2.4348\n",
            "Discrete prompt: ['sul', '##tory', 'illustrations', 'stunt', 'sexually']\n",
            "Epoch 23/50, Loss: 8.0604, Fluency:5.7468,  Bias:2.3136\n",
            "Discrete prompt: ['sul', '##tory', 'illustrations', 'stunt', 'sexually']\n",
            "Epoch 24/50, Loss: 7.8786, Fluency:5.6785,  Bias:2.2001\n",
            "Discrete prompt: ['sul', \"'\", 'illustrations', 'stunt', 'sexually']\n",
            "Epoch 25/50, Loss: 7.7148, Fluency:5.6160,  Bias:2.0988\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'sexually']\n",
            "Epoch 26/50, Loss: 7.5254, Fluency:5.5322,  Bias:1.9931\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'sexually']\n",
            "Epoch 27/50, Loss: 7.4190, Fluency:5.4965,  Bias:1.9226\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 28/50, Loss: 7.3189, Fluency:5.4541,  Bias:1.8649\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 29/50, Loss: 7.1876, Fluency:5.4157,  Bias:1.7720\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 30/50, Loss: 7.0961, Fluency:5.3915,  Bias:1.7046\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 31/50, Loss: 6.9172, Fluency:5.2741,  Bias:1.6431\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 32/50, Loss: 6.8405, Fluency:5.2363,  Bias:1.6042\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 33/50, Loss: 6.6487, Fluency:5.1411,  Bias:1.5075\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 34/50, Loss: 6.5418, Fluency:5.1111,  Bias:1.4307\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 35/50, Loss: 6.4515, Fluency:5.0541,  Bias:1.3974\n",
            "Discrete prompt: ['sul', \"'\", 'illustrations', 'stunt', 'argued']\n",
            "Epoch 36/50, Loss: 6.3383, Fluency:4.9862,  Bias:1.3521\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 37/50, Loss: 6.2798, Fluency:4.9880,  Bias:1.2918\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 38/50, Loss: 6.1395, Fluency:4.9144,  Bias:1.2251\n",
            "Discrete prompt: ['sul', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 39/50, Loss: 6.0574, Fluency:4.8547,  Bias:1.2026\n",
            "Discrete prompt: ['satisfactory', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 40/50, Loss: 5.9310, Fluency:4.7818,  Bias:1.1493\n",
            "Discrete prompt: ['satisfactory', \"'\", 'simpsons', 'stunt', 'argued']\n",
            "Epoch 41/50, Loss: 5.8557, Fluency:4.7476,  Bias:1.1081\n",
            "Discrete prompt: ['satisfactory', \"'\", 'simpsons', 'stunt', 'couples']\n",
            "Epoch 42/50, Loss: 5.7994, Fluency:4.7019,  Bias:1.0974\n",
            "Discrete prompt: ['satisfactory', \"'\", 'simpsons', 'stunt', 'couples']\n",
            "Epoch 43/50, Loss: 5.6659, Fluency:4.6259,  Bias:1.0400\n",
            "Discrete prompt: ['satisfactory', \"'\", 'simpsons', 'stunt', 'couples']\n",
            "Epoch 44/50, Loss: 5.5936, Fluency:4.5881,  Bias:1.0055\n",
            "Discrete prompt: ['satisfactory', \"'\", 'simpsons', 'stunt', 'couples']\n",
            "Epoch 45/50, Loss: 5.5414, Fluency:4.5346,  Bias:1.0068\n",
            "Discrete prompt: ['satisfactory', \"'\", 'simpsons', 'stunt', 'couples']\n",
            "Epoch 46/50, Loss: 5.5885, Fluency:4.6225,  Bias:0.9660\n",
            "Discrete prompt: ['satisfactory', \"'\", 'character', 'stunt', 'couples']\n",
            "Epoch 47/50, Loss: 5.4611, Fluency:4.5197,  Bias:0.9414\n",
            "Discrete prompt: ['satisfactory', \"'\", 'character', 'stunt', 'couples']\n",
            "Epoch 48/50, Loss: 5.3845, Fluency:4.4545,  Bias:0.9300\n",
            "Discrete prompt: ['satisfactory', \"'\", 'character', 'stunt', 'couples']\n",
            "Epoch 49/50, Loss: 5.3370, Fluency:4.4180,  Bias:0.9189\n",
            "Discrete prompt: ['satisfactory', \"'\", 'character', 'stunt', 'couples']\n",
            "Epoch 50/50, Loss: 5.2480, Fluency:4.3690,  Bias:0.8790\n",
            "Discrete prompt: ['satisfactory', \"'\", 'character', 'stunt', 'couples']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_tokens = torch.argmax(soft_prompt, dim=-1)  # [prompt_length]\n",
        "tokens = top_tokens.unsqueeze(0)  # shape: [1, prompt_length]\n",
        "\n",
        "# Get embedding layer\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(tokens).squeeze(0)  # shape: [prompt_length, hidden_size]\n",
        "\n",
        "# Save to file (e.g., .pt or .npy)\n",
        "torch.save(embeddings, \"2_custom_embedding(5).pt\")"
      ],
      "metadata": {
        "id": "FR5VAVD_-Di2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Experiments (Similarity and Initialization)"
      ],
      "metadata": {
        "id": "T0sNq-baHQGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_text = \"You are an unbiased person\"\n",
        "init_ids = tokenizer.encode(init_text, add_special_tokens=False)\n",
        "print(init_ids)\n",
        "print(\"Token count:\", len(init_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMWnUD1yGxQa",
        "outputId": "2c79a247-a944-4b51-cfba-842313756f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2017, 2024, 2019, 4895, 11607, 6924, 2711]\n",
            "Token count: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.vocab_size\n",
        "prompt_length =12\n",
        "soft_prompt_init = torch.zeros(prompt_length, vocab_size, device=device)"
      ],
      "metadata": {
        "id": "KbCxH2NNHd6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode first few tokens\n",
        "for i, token_id in enumerate(init_ids):\n",
        "    soft_prompt_init[i, token_id] = 2.0  # logit to push toward that token\n",
        "\n",
        "# Initialize remaining rows with small random values\n",
        "num_rand = prompt_length - len(init_ids)\n",
        "if num_rand > 0:\n",
        "    soft_prompt_init[len(init_ids):] = torch.randn(num_rand, vocab_size, device=device) * 0.01\n"
      ],
      "metadata": {
        "id": "0rFtmCwuLTYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft_prompt = nn.Parameter(soft_prompt_init, requires_grad=True)\n",
        "optimizer = CustomAdam([soft_prompt], lr=5e-3)\n",
        "EPOCHS = 20\n",
        "soft_prompt = prompt_constructer(EPOCHS,optimizer,train_loader,soft_prompt,prompt_length)"
      ],
      "metadata": {
        "id": "zlHkPkL_B2Yc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ae2669-7264-4779-a171-2308191d5d6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 12.1892, Fluency:6.9110,  Bias:5.2782\n",
            "Discrete prompt: ['you', 'are', 'an', 'un', '##bia', '##sed', 'person', 'macleod', 'answering', '##mbling', '##ful', 'rfc']\n",
            "Epoch 2/20, Loss: 11.2288, Fluency:6.8066,  Bias:4.4222\n",
            "Discrete prompt: ['you', 'are', 'an', 'un', '##bia', '##sed', 'person', '##hui', '##hine', '##mbling', '##some', 'rfc']\n",
            "Epoch 3/20, Loss: 10.4625, Fluency:6.7593,  Bias:3.7032\n",
            "Discrete prompt: ['you', 'are', 'an', 'un', '##bia', '##sed', 'person', 'hacking', '##hine', 'such', '##some', 'rfc']\n",
            "Epoch 4/20, Loss: 10.0434, Fluency:6.7024,  Bias:3.3410\n",
            "Discrete prompt: ['you', 'are', 'an', 'un', '##bia', '##sed', 'person', 'overseas', '##free', '!', '##some', 'rfc']\n",
            "Epoch 5/20, Loss: 9.6736, Fluency:6.6919,  Bias:2.9818\n",
            "Discrete prompt: ['you', 'score', 'an', 'un', 'verlag', '##sed', 'person', 'overseas', '##free', '!', '##some', 'rfc']\n",
            "Epoch 6/20, Loss: 9.2220, Fluency:6.5647,  Bias:2.6573\n",
            "Discrete prompt: ['you', 'wheelbase', 'an', 'registration', 'verlag', '##sed', 'person', 'overseas', 'maui', '!', '##some', 'rfc']\n",
            "Epoch 7/20, Loss: 8.8845, Fluency:6.5471,  Bias:2.3374\n",
            "Discrete prompt: ['encryption', 'wheelbase', 'an', 'registration', 'verlag', '##sed', 'person', 'overseas', 'maui', '!', '##some', 'canceled']\n",
            "Epoch 8/20, Loss: 8.5930, Fluency:6.5853,  Bias:2.0077\n",
            "Discrete prompt: ['unavailable', 'wheelbase', 'an', 'websites', 'cry', '##sed', 'person', 'yearning', 'maui', '!', '##ead', 'canceled']\n",
            "Epoch 9/20, Loss: 8.2212, Fluency:6.4607,  Bias:1.7605\n",
            "Discrete prompt: ['unavailable', 'wheelbase', 'an', 'simi', 'cry', '##sed', 'maui', 'yearning', 'maui', 'so', '##ead', 'canceled']\n",
            "Epoch 10/20, Loss: 7.9603, Fluency:6.4756,  Bias:1.4847\n",
            "Discrete prompt: ['unavailable', '##oya', 'an', 'simi', 'cry', 'aide', 'maui', 'yearning', 'hawaii', 'so', '##ead', 'sherry']\n",
            "Epoch 11/20, Loss: 7.7154, Fluency:6.4269,  Bias:1.2885\n",
            "Discrete prompt: ['unavailable', '##oya', 'an', 'simi', 'cry', 'aide', 'maui', 'yearning', 'hawaii', 'so', '##ead', 'sherry']\n",
            "Epoch 12/20, Loss: 7.4373, Fluency:6.3305,  Bias:1.1068\n",
            "Discrete prompt: ['unavailable', '##oya', 'an', 'simi', 'cry', 'aide', 'maui', 'maui', 'hawaii', 'so', '##ead', 'sherry']\n",
            "Epoch 13/20, Loss: 7.3133, Fluency:6.3689,  Bias:0.9443\n",
            "Discrete prompt: ['broadband', '##oya', 'an', 'simi', 'lina', 'aide', 'maui', 'maui', 'fox', 'whose', '##ead', 'sherry']\n",
            "Epoch 14/20, Loss: 7.0555, Fluency:6.2369,  Bias:0.8186\n",
            "Discrete prompt: ['broadband', '##oya', 'an', 'simi', 'lina', 'aide', 'maui', 'maui', 'fox', 'whose', 'pea', 'sherry']\n",
            "Epoch 15/20, Loss: 6.9425, Fluency:6.2383,  Bias:0.7042\n",
            "Discrete prompt: ['broadband', '##oya', '##nob', 'simi', 'lina', 'aide', 'maui', 'maui', 'fox', 'yet', 'tomato', 'sherry']\n",
            "Epoch 16/20, Loss: 6.8653, Fluency:6.2493,  Bias:0.6160\n",
            "Discrete prompt: ['broadband', 'wheelbase', '##nob', 'simi', 'ken', 'sterling', 'maui', 'maui', 'fox', 'yet', 'tomato', 'patty']\n",
            "Epoch 17/20, Loss: 6.6374, Fluency:6.0876,  Bias:0.5498\n",
            "Discrete prompt: ['broadband', 'wheelbase', '##nob', 'simi', 'ken', 'honda', 'homer', 'maui', 'fox', 'yet', 'tomato', 'patty']\n",
            "Epoch 18/20, Loss: 6.5907, Fluency:6.1003,  Bias:0.4904\n",
            "Discrete prompt: ['broadband', 'wheelbase', '##nob', 'simi', 'ken', 'honda', 'homer', 'maui', 'fox', '@', 'tomato', 'barney']\n",
            "Epoch 19/20, Loss: 6.4694, Fluency:6.0265,  Bias:0.4429\n",
            "Discrete prompt: ['broadband', 'wheelbase', 'magnet', 'simi', 'ken', 'honda', 'homer', 'tory', 'jennings', '@', 'pea', 'barney']\n",
            "Epoch 20/20, Loss: 6.4331, Fluency:6.0167,  Bias:0.4164\n",
            "Discrete prompt: ['broadband', 'wheelbase', 'magnet', 'simi', 'ken', 'honda', 'homer', 'tory', 'jennings', '@', 'pea', 'barney']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_text = \"You are an unbiased person who does not discriminate against people based on their gender\"\n",
        "init_ids = tokenizer.encode(init_text, add_special_tokens=False)\n",
        "print(\"Token count:\", len(init_ids))\n",
        "prompt_length = len(init_ids)\n",
        "soft_prompt_init = torch.zeros(prompt_length, vocab_size, device=device)\n",
        "# One-hot encode first tokens\n",
        "for i, token_id in enumerate(init_ids):\n",
        "    soft_prompt_init[i, token_id] = 1  # logit to push toward that token\n",
        "\n",
        "soft_prompt = nn.Parameter(soft_prompt_init, requires_grad=True)\n",
        "optimizer = CustomAdam([soft_prompt], lr=5e-3)\n",
        "EPOCHS = 20\n",
        "soft_prompt = prompt_constructer(EPOCHS,optimizer,train_loader,soft_prompt,prompt_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "HlUit1PhNQYr",
        "outputId": "2eedb7a7-26cc-4529-986f-f5de067ad219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 19\n",
            "Epoch 1/20, Loss: 11.9891, Fluency:6.9373,  Bias:5.0518\n",
            "Discrete prompt: ['you', 'are', 'an', 'un', '##bia', '##sed', 'person', 'who', 'does', 'not', 'disc', '##rim', '##inate', 'against', 'people', 'based', 'on', 'their', 'gender']\n",
            "Epoch 2/20, Loss: 10.7290, Fluency:6.9589,  Bias:3.7701\n",
            "Discrete prompt: ['you', '##ath', 'an', 'un', '##bia', 'amanda', '##creen', 'who', 'does', 'not', 'junk', '##rim', '##inate', 'against', 'people', 'based', 'on', 'their', 'gender']\n",
            "Epoch 3/20, Loss: 9.8068, Fluency:6.9253,  Bias:2.8815\n",
            "Discrete prompt: ['interceptor', 'gearbox', 'an', 'luna', 'swat', 'rhea', 'ava', 'vain', 'rhea', 'not', 'based', '##rim', 'livingstone', 'against', 'cree', 'based', 'on', 'their', 'gender']\n",
            "Epoch 4/20, Loss: 8.9463, Fluency:6.7723,  Bias:2.1741\n",
            "Discrete prompt: ['geographically', '##ray', 'an', 'luna', 'plug', 'lan', 'ava', 'vain', 'dyed', 'not', 'aunt', '##rim', 'livingstone', 'against', 'colleague', 'based', 'on', 'their', 'gender']\n",
            "Epoch 5/20, Loss: 8.4419, Fluency:6.7280,  Bias:1.7139\n",
            "Discrete prompt: ['geographically', '##ray', 'an', 'ara', 'plug', 'lan', 'vain', 'vain', 'dreamer', 'not', 'aunt', 'livingstone', 'yahoo', 'against', 'colleague', 'based', 'on', '##fin', 'gender']\n",
            "Epoch 6/20, Loss: 8.0167, Fluency:6.6652,  Bias:1.3515\n",
            "Discrete prompt: ['geographically', '##ray', 'an', 'browser', 'lan', 'lan', 'vain', '##scent', 'rhea', 'not', 'aunt', 'fiercely', '##dome', 'against', 'colleague', 'based', 'on', '##fin', 'gender']\n",
            "Epoch 7/20, Loss: 7.6868, Fluency:6.6186,  Bias:1.0682\n",
            "Discrete prompt: ['affirmative', '##ray', 'an', 'browser', 'compact', 'plug', 'vain', '##scent', 'un', 'hawke', 'ava', 'fiercely', '##dome', 'against', 'tory', 'manly', 'on', '##fin', '##bread']\n",
            "Epoch 8/20, Loss: 7.4472, Fluency:6.5679,  Bias:0.8793\n",
            "Discrete prompt: ['yellowish', '##ray', 'an', 'browser', 'plush', 'fiercely', 'overseas', '##grave', 'un', 'hawke', 'ava', 'fiercely', '##ntly', 'against', 'tory', 'manly', 'on', '##fin', '##bread']\n",
            "Epoch 9/20, Loss: 7.3104, Fluency:6.5569,  Bias:0.7536\n",
            "Discrete prompt: ['yellowish', '##ray', 'an', 'browser', 'plush', 'fiercely', 'odi', '##grave', 'un', 'hawke', '##ount', 'fiercely', '##dome', 'against', 'tory', 'manly', 'manly', 'manly', '##bread']\n",
            "Epoch 10/20, Loss: 7.1340, Fluency:6.4956,  Bias:0.6385\n",
            "Discrete prompt: ['yellowish', '##ray', 'an', 'browser', 'plush', 'frantically', '##lessly', '##less', 'un', 'forth', '##ount', 'manly', '##eric', 'needy', 'tory', 'manly', 'manly', 'manly', '##bread']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-50-2297125574.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoft_prompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msoft_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_constructer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msoft_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprompt_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-28-1315114656.py\u001b[0m in \u001b[0;36mprompt_constructer\u001b[0;34m(EPOCHS, optimizer, train_loader, soft_prompt, prompt_length)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mtotal_causal_fluency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m           '''\n\u001b[1;32m     14\u001b[0m           \u001b[0;31m# Debug: Print device info for all tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2808\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2803\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2788\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   2789\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRowFormat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_text = \"You are an unbiased person who does not discriminate against people based on their gender\"\n",
        "init_ids = tokenizer.encode(init_text, add_special_tokens=False)\n",
        "print(\"Token count:\", len(init_ids))\n",
        "prompt_length = len(init_ids)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "soft_prompt_init = torch.zeros(prompt_length, vocab_size, device=device)\n",
        "# One-hot encode first tokens\n",
        "for i, token_id in enumerate(init_ids):\n",
        "    soft_prompt_init[i, token_id] = 2  # logit to push toward that token\n",
        "\n",
        "soft_prompt = nn.Parameter(soft_prompt_init, requires_grad=True)\n",
        "optimizer = CustomAdam([soft_prompt], lr=5e-3)\n",
        "EPOCHS = 20\n",
        "soft_prompt = ref_prompt_constructer(EPOCHS,optimizer,train_loader,soft_prompt,prompt_length, init_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "mTPkgMKwywEE",
        "outputId": "35f25892-d285-4c4e-86b7-53fecc9f6c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 19\n",
            "Epoch 1/20, Loss: 11.6906, Fluency:5.1867,  Bias:4.6257, Similar:1.8781 \n",
            "Discrete prompt: ['you', 'are', 'an', 'un', '##bia', '##sed', 'person', 'who', 'does', 'not', 'disc', '##rim', '##inate', 'against', 'people', 'based', 'on', 'their', 'gender']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-2443499342.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msoft_prompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msoft_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_prompt_constructer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msoft_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprompt_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-19-1808515513.py\u001b[0m in \u001b[0;36minit_prompt_constructer\u001b[0;34m(EPOCHS, optimizer, train_loader, soft_prompt, prompt_length, init_text)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mtotal_causal_fluency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m           '''\n\u001b[1;32m     14\u001b[0m           \u001b[0;31m# Debug: Print device info for all tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2806\u001b[0m         \u001b[0;34m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2807\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2808\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2803\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2787\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2788\u001b[0;31m         formatted_output = format_table(\n\u001b[0m\u001b[1;32m   2789\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mpython_formatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat_columns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRowFormat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mformat_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLazyBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mextract_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextract_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_tokens = torch.argmax(soft_prompt, dim=-1)  # [prompt_length]\n",
        "tokens = top_tokens.unsqueeze(0)  # shape: [1, prompt_length]\n",
        "\n",
        "# Get embedding layer\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(tokens).squeeze(0)  # shape: [prompt_length, hidden_size]\n",
        "\n",
        "# Save to file (e.g., .pt or .npy)\n",
        "torch.save(embeddings, \"new_embedding_fluency_similar.pt\")"
      ],
      "metadata": {
        "id": "Jj5qRR_7FxCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ref_text = \"fair gender equal profession male female unbiased\"\n",
        "ref_ids = tokenizer.encode(ref_text, add_special_tokens=False)\n",
        "print(\"Token count:\", len(ref_ids))\n",
        "prompt_length = len(ref_ids)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "soft_prompt_init = torch.randn(prompt_length, vocab_size, device=device) * 0.01\n",
        "\n",
        "# One-hot encode first tokens\n",
        "for i, token_id in enumerate(ref_ids):\n",
        "    soft_prompt_init[i, token_id] = 2  # logit to push toward that token\n",
        "\n",
        "soft_prompt = nn.Parameter(soft_prompt_init, requires_grad=True)\n",
        "optimizer = CustomAdam([soft_prompt], lr=4e-3)\n",
        "EPOCHS = 100\n",
        "soft_prompt = ref_prompt_constructer(EPOCHS,optimizer,train_loader,soft_prompt,prompt_length, ref_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5qgcooDImCO",
        "outputId": "da43a628-d24c-42f6-fefe-7bf7b7b337a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 9\n",
            "Epoch 1/100, Loss: 12.4801, Fluency:3.4115,  Bias:6.2230, Similar:2.8456 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'profession', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 2/100, Loss: 11.6268, Fluency:3.4089,  Bias:5.5299, Similar:2.6881 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'profession', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 3/100, Loss: 10.9498, Fluency:3.4151,  Bias:4.9373, Similar:2.5973 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'profession', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 4/100, Loss: 10.4812, Fluency:3.4066,  Bias:4.5558, Similar:2.5188 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'profession', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 5/100, Loss: 10.0339, Fluency:3.3869,  Bias:4.2052, Similar:2.4417 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'profession', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 6/100, Loss: 9.6922, Fluency:3.3838,  Bias:3.9451, Similar:2.3633 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'wakefield', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 7/100, Loss: 9.3116, Fluency:3.3621,  Bias:3.6702, Similar:2.2793 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'wakefield', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 8/100, Loss: 9.0280, Fluency:3.3445,  Bias:3.4820, Similar:2.2015 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'wakefield', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 9/100, Loss: 8.7625, Fluency:3.3597,  Bias:3.2672, Similar:2.1356 \n",
            "Discrete prompt: ['fair', 'gender', 'equal', 'wakefield', 'male', 'female', 'un', '##bia', '##sed']\n",
            "Epoch 10/100, Loss: 8.5962, Fluency:3.3477,  Bias:3.1638, Similar:2.0847 \n",
            "Discrete prompt: ['fair', 'gender', 'companion', 'wakefield', '##pins', 'female', 'un', 'harper', '##sed']\n",
            "Epoch 11/100, Loss: 8.4193, Fluency:3.3596,  Bias:3.0151, Similar:2.0447 \n",
            "Discrete prompt: ['fair', 'gender', 'companion', 'boeing', '##pins', 'female', 'un', 'yorkshire', '##sed']\n",
            "Epoch 12/100, Loss: 8.1794, Fluency:3.3612,  Bias:2.8083, Similar:2.0098 \n",
            "Discrete prompt: ['fair', 'gender', 'companion', 'boeing', '##pins', 'female', 'un', 'yorkshire', '##sed']\n",
            "Epoch 13/100, Loss: 7.9968, Fluency:3.3043,  Bias:2.7196, Similar:1.9729 \n",
            "Discrete prompt: ['throttle', 'gender', 'ham', 'boeing', 'haute', 'female', 'un', 'yorkshire', '##sed']\n",
            "Epoch 14/100, Loss: 7.8305, Fluency:3.3209,  Bias:2.5763, Similar:1.9333 \n",
            "Discrete prompt: ['throttle', 'gender', 'ham', 'boeing', 'haute', 'female', 'un', 'yorkshire', '##bread']\n",
            "Epoch 15/100, Loss: 7.7079, Fluency:3.3362,  Bias:2.4781, Similar:1.8936 \n",
            "Discrete prompt: ['throttle', 'gender', 'ham', 'wakefield', 'haute', 'female', 'un', 'yorkshire', '##bread']\n",
            "Epoch 16/100, Loss: 7.5095, Fluency:3.2922,  Bias:2.3610, Similar:1.8562 \n",
            "Discrete prompt: ['throttle', 'gender', 'ham', 'wakefield', 'haute', 'female', 'un', 'yorkshire', 'bun']\n",
            "Epoch 17/100, Loss: 7.3292, Fluency:3.3171,  Bias:2.1908, Similar:1.8213 \n",
            "Discrete prompt: ['throttle', 'gender', 'ham', 'wakefield', 'haute', 'female', 'un', 'yorkshire', 'bun']\n",
            "Epoch 18/100, Loss: 7.1435, Fluency:3.2828,  Bias:2.0685, Similar:1.7923 \n",
            "Discrete prompt: ['throttle', 'gender', 'ham', 'wakefield', 'haute', 'female', 'un', 'ow', '##leader']\n",
            "Epoch 19/100, Loss: 7.0175, Fluency:3.2852,  Bias:1.9647, Similar:1.7676 \n",
            "Discrete prompt: ['newt', 'eireann', 'ham', 'wakefield', 'haute', 'female', 'un', 'ow', 'bunch']\n",
            "Epoch 20/100, Loss: 6.8489, Fluency:3.2582,  Bias:1.8471, Similar:1.7436 \n",
            "Discrete prompt: ['newt', 'eireann', 'ham', 'wakefield', '##kow', 'female', 'un', 'ow', 'bunch']\n",
            "Epoch 21/100, Loss: 6.7361, Fluency:3.2632,  Bias:1.7518, Similar:1.7211 \n",
            "Discrete prompt: ['newt', 'eireann', 'ham', 'wakefield', '##kow', 'durable', 'shore', 'ow', 'bunch']\n",
            "Epoch 22/100, Loss: 6.5874, Fluency:3.2610,  Bias:1.6265, Similar:1.6999 \n",
            "Discrete prompt: ['newt', 'eireann', 'ham', 'wakefield', '##kow', 'durable', 'shore', 'ow', 'bunch']\n",
            "Epoch 23/100, Loss: 6.4667, Fluency:3.2426,  Bias:1.5438, Similar:1.6803 \n",
            "Discrete prompt: ['newt', 'eireann', 'ham', 'wakefield', 'zoe', 'durable', 'shore', 'ow', 'bunch']\n",
            "Epoch 24/100, Loss: 6.3154, Fluency:3.2464,  Bias:1.4081, Similar:1.6609 \n",
            "Discrete prompt: ['newt', 'eireann', 'rowe', 'wakefield', 'zoe', 'durable', 'shore', 'ow', 'bunch']\n",
            "Epoch 25/100, Loss: 6.1993, Fluency:3.2436,  Bias:1.3145, Similar:1.6412 \n",
            "Discrete prompt: ['##thal', 'eireann', 'rowe', 'wakefield', 'zoe', 'durable', 'shore', 'ow', 'bunch']\n",
            "Epoch 26/100, Loss: 6.0579, Fluency:3.2311,  Bias:1.2048, Similar:1.6220 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'durable', 'fabulous', 'ow', 'bunch']\n",
            "Epoch 27/100, Loss: 5.9426, Fluency:3.2190,  Bias:1.1201, Similar:1.6035 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'durable', 'hacking', 'ow', 'bunch']\n",
            "Epoch 28/100, Loss: 5.8396, Fluency:3.2029,  Bias:1.0518, Similar:1.5849 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'swimmer', 'hacking', 'ow', 'bunch']\n",
            "Epoch 29/100, Loss: 5.7558, Fluency:3.2147,  Bias:0.9738, Similar:1.5673 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'swimmer', 'hacking', 'samoa', 'bunch']\n",
            "Epoch 30/100, Loss: 5.6604, Fluency:3.1954,  Bias:0.9158, Similar:1.5492 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'swimmer', 'hacking', 'samoa', '##leader']\n",
            "Epoch 31/100, Loss: 5.5762, Fluency:3.1973,  Bias:0.8483, Similar:1.5306 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'swimmer', 'hacking', 'samoa', 'bunch']\n",
            "Epoch 32/100, Loss: 5.4950, Fluency:3.1762,  Bias:0.8065, Similar:1.5124 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'swimmer', 'hacking', 'samoa', 'bunch']\n",
            "Epoch 33/100, Loss: 5.4193, Fluency:3.1505,  Bias:0.7735, Similar:1.4953 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'wakefield', 'zoe', 'swimmer', 'hacking', 'samoa', 'bunch']\n",
            "Epoch 34/100, Loss: 5.3601, Fluency:3.1488,  Bias:0.7330, Similar:1.4782 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', 'zoe', 'swimmer', 'hacking', 'samoa', 'bunch']\n",
            "Epoch 35/100, Loss: 5.3018, Fluency:3.1523,  Bias:0.6887, Similar:1.4608 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', 'zoe', 'swimmer', 'hacking', 'samoa', 'bunch']\n",
            "Epoch 36/100, Loss: 5.2368, Fluency:3.1452,  Bias:0.6474, Similar:1.4442 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', 'zoe', 'swimmer', 'hacking', 'samoa', 'bunch']\n",
            "Epoch 37/100, Loss: 5.1904, Fluency:3.1451,  Bias:0.6174, Similar:1.4279 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', 'zoe', 'swimmer', 'hacking', 'samoa', '##bread']\n",
            "Epoch 38/100, Loss: 5.1264, Fluency:3.1363,  Bias:0.5782, Similar:1.4119 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', 'zoe', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 39/100, Loss: 5.0940, Fluency:3.1472,  Bias:0.5508, Similar:1.3960 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', 'zoe', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 40/100, Loss: 5.0159, Fluency:3.1005,  Bias:0.5353, Similar:1.3802 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', 'zoe', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 41/100, Loss: 4.9688, Fluency:3.1025,  Bias:0.5014, Similar:1.3649 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 42/100, Loss: 4.9626, Fluency:3.1296,  Bias:0.4828, Similar:1.3502 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 43/100, Loss: 4.8776, Fluency:3.0824,  Bias:0.4593, Similar:1.3358 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 44/100, Loss: 4.8746, Fluency:3.1107,  Bias:0.4419, Similar:1.3220 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 45/100, Loss: 4.8349, Fluency:3.1094,  Bias:0.4168, Similar:1.3086 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 46/100, Loss: 4.7826, Fluency:3.0854,  Bias:0.4017, Similar:1.2955 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 47/100, Loss: 4.7199, Fluency:3.0555,  Bias:0.3818, Similar:1.2826 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 48/100, Loss: 4.7203, Fluency:3.0755,  Bias:0.3753, Similar:1.2696 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 49/100, Loss: 4.6633, Fluency:3.0422,  Bias:0.3638, Similar:1.2573 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 50/100, Loss: 4.6169, Fluency:3.0258,  Bias:0.3458, Similar:1.2454 \n",
            "Discrete prompt: ['##thal', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 51/100, Loss: 4.5926, Fluency:3.0218,  Bias:0.3371, Similar:1.2336 \n",
            "Discrete prompt: ['sul', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 52/100, Loss: 4.5857, Fluency:3.0341,  Bias:0.3293, Similar:1.2222 \n",
            "Discrete prompt: ['sul', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'samoa', '##bread']\n",
            "Epoch 53/100, Loss: 4.5604, Fluency:3.0254,  Bias:0.3240, Similar:1.2110 \n",
            "Discrete prompt: ['sul', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'blanc', '##bread']\n",
            "Epoch 54/100, Loss: 4.5392, Fluency:3.0276,  Bias:0.3110, Similar:1.2006 \n",
            "Discrete prompt: ['sul', 'eireann', 'tame', 'olympus', '##sche', 'male', 'hacking', 'blanc', '##tail']\n",
            "Epoch 55/100, Loss: 4.4737, Fluency:2.9862,  Bias:0.2973, Similar:1.1902 \n",
            "Discrete prompt: ['sul', 'eireann', 'tame', 'gymnast', '##sche', 'shelley', 'hacking', 'blanc', '##tail']\n",
            "Epoch 56/100, Loss: 4.4704, Fluency:2.9930,  Bias:0.2974, Similar:1.1799 \n",
            "Discrete prompt: ['sul', 'eireann', 'kin', 'gymnast', '##sche', 'shelley', 'hacking', 'blanc', '##tail']\n",
            "Epoch 57/100, Loss: 4.4644, Fluency:3.0039,  Bias:0.2903, Similar:1.1701 \n",
            "Discrete prompt: ['sul', 'eireann', 'kin', 'gymnast', '##sche', 'shelley', 'hacking', 'blanc', '##tail']\n",
            "Epoch 58/100, Loss: 4.4318, Fluency:2.9848,  Bias:0.2864, Similar:1.1606 \n",
            "Discrete prompt: ['sul', 'eireann', 'kin', 'gymnast', '##sche', 'shelley', 'hacking', 'blanc', '##tail']\n",
            "Epoch 59/100, Loss: 4.4073, Fluency:2.9756,  Bias:0.2807, Similar:1.1510 \n",
            "Discrete prompt: ['sul', 'eireann', 'kin', 'gymnast', '##sche', 'shelley', 'hacking', 'blanc', '##tail']\n",
            "Epoch 60/100, Loss: 4.3596, Fluency:2.9466,  Bias:0.2713, Similar:1.1417 \n",
            "Discrete prompt: ['sul', 'eireann', 'kin', 'gymnast', '##sche', '$', 'hacking', 'blanc', '##tail']\n",
            "Epoch 61/100, Loss: 4.3775, Fluency:2.9735,  Bias:0.2718, Similar:1.1322 \n",
            "Discrete prompt: ['sul', 'eireann', 'kin', 'gymnast', '##sche', '$', 'hacking', 'blanc', '##tail']\n",
            "Epoch 62/100, Loss: 4.3394, Fluency:2.9506,  Bias:0.2662, Similar:1.1226 \n",
            "Discrete prompt: ['sul', 'eireann', 'speed', 'gymnast', '##sche', '$', 'hacking', 'blanc', '##tail']\n",
            "Epoch 63/100, Loss: 4.3118, Fluency:2.9359,  Bias:0.2624, Similar:1.1134 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'gymnast', '##sche', '$', 'hacking', 'blanc', '##tail']\n",
            "Epoch 64/100, Loss: 4.3072, Fluency:2.9436,  Bias:0.2593, Similar:1.1043 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'gymnast', '##sche', '$', 'hacking', 'blanc', '##tail']\n",
            "Epoch 65/100, Loss: 4.2705, Fluency:2.9182,  Bias:0.2582, Similar:1.0941 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'television', '##sche', '$', 'hacking', 'blanc', '##tail']\n",
            "Epoch 66/100, Loss: 4.2530, Fluency:2.9170,  Bias:0.2530, Similar:1.0830 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'television', 'compares', '$', 'hacking', 'pear', '##tail']\n",
            "Epoch 67/100, Loss: 4.2295, Fluency:2.9082,  Bias:0.2493, Similar:1.0720 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'television', 'compares', '$', 'hacking', 'pear', '##tail']\n",
            "Epoch 68/100, Loss: 4.2084, Fluency:2.8974,  Bias:0.2493, Similar:1.0617 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'television', 'compares', '$', 'hacking', 'pear', '##tail']\n",
            "Epoch 69/100, Loss: 4.1812, Fluency:2.8804,  Bias:0.2488, Similar:1.0520 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'television', 'compares', '$', 'hacking', 'pear', '##tail']\n",
            "Epoch 70/100, Loss: 4.1577, Fluency:2.8687,  Bias:0.2468, Similar:1.0422 \n",
            "Discrete prompt: ['sul', 'aquatics', 'speed', 'television', 'compares', '$', 'conan', 'pear', '##tail']\n",
            "Epoch 71/100, Loss: 4.1254, Fluency:2.8532,  Bias:0.2395, Similar:1.0327 \n",
            "Discrete prompt: ['sul', '’', 'speed', 'television', 'compares', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 72/100, Loss: 4.1239, Fluency:2.8582,  Bias:0.2425, Similar:1.0232 \n",
            "Discrete prompt: ['sul', '’', 'speed', 'television', 'compares', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 73/100, Loss: 4.0926, Fluency:2.8419,  Bias:0.2368, Similar:1.0139 \n",
            "Discrete prompt: ['sul', '’', 'speed', 'television', 'compares', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 74/100, Loss: 4.1050, Fluency:2.8585,  Bias:0.2421, Similar:1.0045 \n",
            "Discrete prompt: ['sul', '’', 'speed', 'television', 'compares', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 75/100, Loss: 4.0570, Fluency:2.8238,  Bias:0.2365, Similar:0.9967 \n",
            "Discrete prompt: ['sul', '’', 'speed', 'television', 'compares', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 76/100, Loss: 4.0455, Fluency:2.8227,  Bias:0.2336, Similar:0.9892 \n",
            "Discrete prompt: ['sul', '’', 'speed', 'television', 'compares', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 77/100, Loss: 4.0413, Fluency:2.8218,  Bias:0.2378, Similar:0.9816 \n",
            "Discrete prompt: ['sul', '’', 'speed', '<', 'compares', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 78/100, Loss: 4.0251, Fluency:2.8183,  Bias:0.2322, Similar:0.9745 \n",
            "Discrete prompt: ['sul', '’', 'speed', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 79/100, Loss: 4.0112, Fluency:2.8181,  Bias:0.2249, Similar:0.9682 \n",
            "Discrete prompt: ['sul', '’', 'speed', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 80/100, Loss: 3.9717, Fluency:2.7849,  Bias:0.2254, Similar:0.9615 \n",
            "Discrete prompt: ['sul', '’', 'fatal', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 81/100, Loss: 3.9807, Fluency:2.7995,  Bias:0.2262, Similar:0.9549 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 82/100, Loss: 3.9611, Fluency:2.7905,  Bias:0.2219, Similar:0.9488 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 83/100, Loss: 3.9370, Fluency:2.7703,  Bias:0.2234, Similar:0.9432 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 84/100, Loss: 3.9154, Fluency:2.7586,  Bias:0.2191, Similar:0.9376 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 85/100, Loss: 3.9250, Fluency:2.7751,  Bias:0.2181, Similar:0.9318 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 86/100, Loss: 3.9091, Fluency:2.7655,  Bias:0.2172, Similar:0.9264 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'conan', 'pear', 'corps']\n",
            "Epoch 87/100, Loss: 3.8979, Fluency:2.7570,  Bias:0.2197, Similar:0.9212 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'cow', 'pear', 'corps']\n",
            "Epoch 88/100, Loss: 3.8922, Fluency:2.7636,  Bias:0.2126, Similar:0.9160 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', 'corps']\n",
            "Epoch 89/100, Loss: 3.8741, Fluency:2.7476,  Bias:0.2151, Similar:0.9114 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', 'corps']\n",
            "Epoch 90/100, Loss: 3.8693, Fluency:2.7471,  Bias:0.2158, Similar:0.9064 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', 'corps']\n",
            "Epoch 91/100, Loss: 3.8675, Fluency:2.7437,  Bias:0.2224, Similar:0.9014 \n",
            "Discrete prompt: ['sul', '’', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', 'corps']\n",
            "Epoch 92/100, Loss: 3.8328, Fluency:2.7256,  Bias:0.2098, Similar:0.8974 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', 'corps']\n",
            "Epoch 93/100, Loss: 3.8446, Fluency:2.7393,  Bias:0.2118, Similar:0.8934 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', '(']\n",
            "Epoch 94/100, Loss: 3.8276, Fluency:2.7310,  Bias:0.2072, Similar:0.8893 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', '(']\n",
            "Epoch 95/100, Loss: 3.8351, Fluency:2.7385,  Bias:0.2110, Similar:0.8855 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', '(']\n",
            "Epoch 96/100, Loss: 3.7996, Fluency:2.7097,  Bias:0.2077, Similar:0.8821 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'yoko', 'pear', '(']\n",
            "Epoch 97/100, Loss: 3.7983, Fluency:2.7137,  Bias:0.2059, Similar:0.8788 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'male', 'pear', '(']\n",
            "Epoch 98/100, Loss: 3.7876, Fluency:2.7086,  Bias:0.2036, Similar:0.8755 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'male', 'pear', '(']\n",
            "Epoch 99/100, Loss: 3.7692, Fluency:2.6925,  Bias:0.2044, Similar:0.8723 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'male', 'pear', '(']\n",
            "Epoch 100/100, Loss: 3.7580, Fluency:2.6862,  Bias:0.2023, Similar:0.8695 \n",
            "Discrete prompt: ['sul', '##glass', 'bow', '<', '’', 'mirrored', 'male', 'pear', '(']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_tokens = torch.argmax(soft_prompt, dim=-1)  # [prompt_length]\n",
        "tokens = top_tokens.unsqueeze(0)  # shape: [1, prompt_length]\n",
        "\n",
        "# Get embedding layer\n",
        "embedding_layer = model.get_input_embeddings()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = embedding_layer(tokens).squeeze(0)  # shape: [prompt_length, hidden_size]\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/initalized_embedding_fluency_similar.pt\"\n",
        "torch.save(embeddings, save_path)"
      ],
      "metadata": {
        "id": "pcQ5nMRROlBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "19hpU5veOt7D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}